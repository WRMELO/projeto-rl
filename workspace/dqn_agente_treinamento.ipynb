{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637438ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6f0fcc6",
   "metadata": {},
   "source": [
    "## üéØ Desenvolvimento do Agente DQN para Aloca√ß√£o de Ativos\n",
    "\n",
    "Este notebook implementa a Etapa 6 do projeto de Reinforcement Learning, dedicada √† constru√ß√£o pr√°tica de um agente baseado no algoritmo Deep Q-Network (DQN), aplicado √† aloca√ß√£o din√¢mica de uma carteira com tr√™s ativos: VALE3, PETR4 e BRFS3.\n",
    "\n",
    "O agente ser√° treinado em um ambiente simulado (`PortfolioEnv`), configurado com base nos dados hist√≥ricos processados previamente. A arquitetura ser√° baseada em PyTorch com suporte a CUDA, validando os aprendizados te√≥ricos sobre a fun√ß√£o Q, replay buffer e pol√≠ticas $\\epsilon$-greedy.\n",
    "\n",
    "Etapas implementadas neste notebook:\n",
    "\n",
    "1. Verifica√ß√£o do ambiente e suporte √† GPU;\n",
    "2. Defini√ß√£o da rede neural DQN;\n",
    "3. Constru√ß√£o do ambiente simulado `PortfolioEnv`;\n",
    "4. Configura√ß√£o do buffer de experi√™ncia e par√¢metros de treinamento;\n",
    "5. Execu√ß√£o do treinamento inicial do agente;\n",
    "6. An√°lise dos resultados preliminares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df99822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dispositivo ativo: cuda\n",
      "CUDA dispon√≠vel? True\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas principais para RL e Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Verifica se a GPU est√° dispon√≠vel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"‚úÖ Dispositivo ativo: {device}\")\n",
    "print(f\"CUDA dispon√≠vel? {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a10669",
   "metadata": {},
   "source": [
    "## üß† Arquitetura da Rede Neural DQN\n",
    "\n",
    "A rede neural utilizada neste projeto segue uma arquitetura simples e eficiente, adequada ao tamanho do vetor de estado calculado com base em indicadores t√©cnicos e vari√°veis da carteira.\n",
    "\n",
    "A rede recebe como entrada o vetor de estado do ambiente simulado (`PortfolioEnv`) e retorna os valores Q estimados para cada a√ß√£o poss√≠vel. A estrutura adotada √© composta por:\n",
    "\n",
    "- Camada de entrada: dimens√£o igual ao tamanho total do vetor de estado;\n",
    "- Camada oculta 1: 128 neur√¥nios com ativa√ß√£o ReLU;\n",
    "- Camada oculta 2: 64 neur√¥nios com ativa√ß√£o ReLU;\n",
    "- Camada de sa√≠da: 9 neur√¥nios, correspondentes √†s a√ß√µes discretas poss√≠veis (comprar, vender ou manter) para cada um dos tr√™s ativos.\n",
    "\n",
    "A sa√≠da da rede representa o valor esperado de recompensa (Q-value) para cada a√ß√£o, dada a configura√ß√£o atual do ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f40a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c9bb7",
   "metadata": {},
   "source": [
    "## üåç Ambiente Simulado: `PortfolioEnv`\n",
    "\n",
    "O ambiente `PortfolioEnv` simula o comportamento de um mercado financeiro com tr√™s ativos (VALE3, PETR4, BRFS3), considerando vari√°veis de estado, posi√ß√µes mantidas pelo agente e saldo de caixa dispon√≠vel.\n",
    "\n",
    "A interface segue o padr√£o `gym.Env`, com os principais m√©todos:\n",
    "\n",
    "- `reset()`: reinicializa o ambiente para o primeiro dia √∫til, zera posi√ß√µes e caixa, e retorna o primeiro estado observ√°vel;\n",
    "- `step(action_vector)`: executa as a√ß√µes sobre os ativos, atualiza posi√ß√µes, saldo e calcula a recompensa;\n",
    "- `render()`: imprime informa√ß√µes do estado atual da carteira ‚Äî √∫til para fins de depura√ß√£o e visualiza√ß√£o em tempo real durante o treinamento;\n",
    "- Atributos internos controlam a linha temporal, recompensas acumuladas e hist√≥rico da carteira.\n",
    "\n",
    "As a√ß√µes s√£o vetores discretos com tamanho igual ao n√∫mero de ativos. Para cada ativo, a a√ß√£o pode ser:\n",
    "\n",
    "- `0`: manter posi√ß√£o;\n",
    "- `1`: comprar uma unidade (caso haja saldo);\n",
    "- `2`: vender uma unidade (caso haja posi√ß√£o).\n",
    "\n",
    "A recompensa √© definida com base na varia√ß√£o do valor total da carteira entre `t` e `t+1`, incluindo caixa e valor de mercado das posi√ß√µes. Custos de transa√ß√£o podem ser incorporados como penalidade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e467185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, df, initial_cash=1.0):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "\n",
    "        # DataFrame com os dados de mercado hist√≥ricos (j√° preparados externamente)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "        self.n_assets = 3  # N√∫mero de ativos: VALE3, PETR4, BRFS3\n",
    "        self.initial_cash = initial_cash  # Valor inicial em caixa\n",
    "\n",
    "        # Espa√ßo de a√ß√µes: vetor com 3 entradas (uma por ativo), onde:\n",
    "        # 0 = manter, 1 = comprar, 2 = vender\n",
    "        self.action_space = spaces.MultiDiscrete([3] * self.n_assets)\n",
    "\n",
    "        # Inicializa√ß√µes\n",
    "        self.current_step = 0           # √çndice temporal do ambiente\n",
    "        self.cash = self.initial_cash   # Saldo dispon√≠vel\n",
    "        self.positions = [0] * self.n_assets  # Quantidade de cada ativo na carteira\n",
    "        self.history = []               # Hist√≥rico de recompensas ou estados, se desejado\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia o ambiente para o primeiro dia √∫til da simula√ß√£o\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.positions = [0] * self.n_assets\n",
    "        self.history.clear()\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Aplica a√ß√µes sobre os ativos e avan√ßa para o pr√≥ximo dia.\n",
    "        A l√≥gica detalhada ser√° implementada posteriormente.\n",
    "        \"\"\"\n",
    "        next_state = self._get_state()\n",
    "        reward = 0.0\n",
    "        done = self.current_step >= len(self.df) - 2  # -2 para garantir que t+1 exista\n",
    "        info = {}\n",
    "        self.current_step += 1\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Retorna o vetor de estado do dia atual, combinando dados de mercado com portf√≥lio\"\"\"\n",
    "        market_data = self.df.iloc[self.current_step].values\n",
    "        return np.concatenate([market_data, self.positions, [self.cash]])\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Exibe o estado atual do ambiente (modo 'human' √© um padr√£o da API gym).\n",
    "        Pode ser ignorado se rodando em scripts automatizados.\n",
    "        \"\"\"\n",
    "        print(f\"Step: {self.current_step} | Caixa: {self.cash:.2f} | Posi√ß√µes: {self.positions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bb90f",
   "metadata": {},
   "source": [
    "## üîÅ L√≥gica de Transi√ß√£o do Ambiente: Implementa√ß√£o do M√©todo `step()`\n",
    "\n",
    "O m√©todo `step()` √© o cora√ß√£o do ambiente `PortfolioEnv`. Ele define como o agente interage com o mercado e como essa intera√ß√£o afeta o estado da carteira. A implementa√ß√£o segue estas etapas:\n",
    "\n",
    "1. **Registro do valor da carteira antes das a√ß√µes**  \n",
    "   Calcula o valor da carteira no tempo `t`, somando o caixa dispon√≠vel e o valor de mercado das posi√ß√µes atuais com os pre√ßos do dia corrente.\n",
    "\n",
    "2. **Execu√ß√£o das a√ß√µes de compra, venda ou manuten√ß√£o**  \n",
    "   Para cada ativo:\n",
    "   - `1` (compra): se houver saldo suficiente, reduz o caixa e incrementa a posi√ß√£o;\n",
    "   - `2` (venda): se houver posi√ß√£o, reduz a quantidade do ativo e adiciona o valor ao caixa;\n",
    "   - `0` (manter): nenhuma altera√ß√£o.\n",
    "\n",
    "   Cada opera√ß√£o de compra ou venda acarreta um **custo de transa√ß√£o fixo**, deduzido do caixa dispon√≠vel. Isso reflete taxas de corretagem, spread e custos operacionais ‚Äî incentivando o agente a evitar opera√ß√µes excessivas.\n",
    "\n",
    "3. **C√°lculo da recompensa**  \n",
    "   Ap√≥s as a√ß√µes, avan√ßa para o pr√≥ximo dia (`t+1`) e calcula o novo valor da carteira.  \n",
    "   A recompensa √© definida como a **varia√ß√£o percentual do valor da carteira** entre `t` e `t+1`.  \n",
    "   Se o epis√≥dio estiver no √∫ltimo dia √∫til (`t+1` n√£o existir), a recompensa √© zero.\n",
    "\n",
    "4. **Atualiza√ß√£o do passo temporal e retorno dos resultados**  \n",
    "   - `next_state`: novo vetor de observa√ß√£o;\n",
    "   - `reward`: feedback para o agente;\n",
    "   - `done`: sinaliza fim do epis√≥dio;\n",
    "   - `info`: dicion√°rio com dados complementares como o valor atual da carteira.\n",
    "\n",
    "Este m√©todo transforma a simula√ß√£o em um ambiente din√¢mico realista, no qual o agente precisa aprender a alocar recursos de forma estrat√©gica e econ√¥mica, ponderando retorno e custo de transa√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cbd5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, action):\n",
    "    \"\"\"\n",
    "    Aplica o vetor de a√ß√µes fornecido (um por ativo), atualiza as posi√ß√µes e o caixa,\n",
    "    avan√ßa o tempo e calcula a recompensa como a varia√ß√£o percentual da carteira l√≠quida.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defini√ß√£o do custo fixo por transa√ß√£o (compra ou venda)\n",
    "    transaction_cost = 0.001  # 0.1% do valor da opera√ß√£o\n",
    "\n",
    "    # Pre√ßos atuais dos ativos (tempo t)\n",
    "    current_prices = self.df.iloc[self.current_step].values\n",
    "\n",
    "    # Valor da carteira antes das a√ß√µes\n",
    "    portfolio_value_before = self.cash + sum([\n",
    "        self.positions[i] * current_prices[i]\n",
    "        for i in range(self.n_assets)\n",
    "    ])\n",
    "\n",
    "    # Aplica√ß√£o das a√ß√µes\n",
    "    for i in range(self.n_assets):\n",
    "        price = current_prices[i]\n",
    "        if action[i] == 1:  # Comprar\n",
    "            total_cost = price * (1 + transaction_cost)\n",
    "            if self.cash >= total_cost:\n",
    "                self.positions[i] += 1\n",
    "                self.cash -= total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26895c9",
   "metadata": {},
   "source": [
    "## ‚úÖ Finaliza√ß√£o do Ambiente Simulado `PortfolioEnv`\n",
    "\n",
    "Com a implementa√ß√£o completa do m√©todo `step()`, o ambiente `PortfolioEnv` est√° agora totalmente funcional e pronto para ser utilizado no treinamento do agente de Reinforcement Learning.\n",
    "\n",
    "Esse ambiente representa, com realismo e controle, um cen√°rio simplificado de mercado no qual o agente:\n",
    "\n",
    "- Observa indicadores t√©cnicos e seu pr√≥prio portf√≥lio (estado);\n",
    "- Decide diariamente se deve comprar, vender ou manter posi√ß√£o (a√ß√£o);\n",
    "- Recebe uma recompensa baseada na evolu√ß√£o da carteira (retorno financeiro ajustado por custo de transa√ß√£o).\n",
    "\n",
    "A inclus√£o de **custos fixos por opera√ß√£o** torna o desafio mais alinhado com o mundo real, penalizando estrat√©gias com excesso de transa√ß√µes e incentivando decis√µes eficientes.\n",
    "\n",
    "Al√©m disso, o ambiente j√° √© compat√≠vel com algoritmos baseados em `gym`, podendo ser diretamente integrado ao loop de treinamento do DQN, Replay Buffer e pol√≠tica de explora√ß√£o Œµ-greedy.\n",
    "\n",
    "A partir deste ponto, seguimos para a configura√ß√£o da estrutura de aprendizado propriamente dita, que inclui:\n",
    "\n",
    "1. Buffer de experi√™ncias (Replay Buffer);\n",
    "2. Pol√≠tica de explora√ß√£o baseada em Œµ-greedy;\n",
    "3. Loop de treinamento com atualiza√ß√£o da rede Q e rede alvo.\n",
    "\n",
    "Com isso, encerramos a Etapa 6.3 ‚Äî *Defini√ß√£o do Ambiente de Simula√ß√£o*.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

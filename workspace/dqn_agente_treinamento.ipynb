{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637438ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6f0fcc6",
   "metadata": {},
   "source": [
    "## üéØ Desenvolvimento do Agente DQN para Aloca√ß√£o de Ativos\n",
    "\n",
    "Este notebook implementa a Etapa 6 do projeto de Reinforcement Learning, dedicada √† constru√ß√£o pr√°tica de um agente baseado no algoritmo Deep Q-Network (DQN), aplicado √† aloca√ß√£o din√¢mica de uma carteira com tr√™s ativos: VALE3, PETR4 e BRFS3.\n",
    "\n",
    "O agente ser√° treinado em um ambiente simulado (`PortfolioEnv`), configurado com base nos dados hist√≥ricos processados previamente. A arquitetura ser√° baseada em PyTorch com suporte a CUDA, validando os aprendizados te√≥ricos sobre a fun√ß√£o Q, replay buffer e pol√≠ticas $\\epsilon$-greedy.\n",
    "\n",
    "Etapas implementadas neste notebook:\n",
    "\n",
    "1. Verifica√ß√£o do ambiente e suporte √† GPU;\n",
    "2. Defini√ß√£o da rede neural DQN;\n",
    "3. Constru√ß√£o do ambiente simulado `PortfolioEnv`;\n",
    "4. Configura√ß√£o do buffer de experi√™ncia e par√¢metros de treinamento;\n",
    "5. Execu√ß√£o do treinamento inicial do agente;\n",
    "6. An√°lise dos resultados preliminares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df99822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dispositivo ativo: cuda\n",
      "CUDA dispon√≠vel? True\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas principais para RL e Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Verifica se a GPU est√° dispon√≠vel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"‚úÖ Dispositivo ativo: {device}\")\n",
    "print(f\"CUDA dispon√≠vel? {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a10669",
   "metadata": {},
   "source": [
    "## üß† Arquitetura da Rede Neural DQN\n",
    "\n",
    "A rede neural utilizada neste projeto segue uma arquitetura simples e eficiente, adequada ao tamanho do vetor de estado calculado com base em indicadores t√©cnicos e vari√°veis da carteira.\n",
    "\n",
    "A rede recebe como entrada o vetor de estado do ambiente simulado (`PortfolioEnv`) e retorna os valores Q estimados para cada a√ß√£o poss√≠vel. A estrutura adotada √© composta por:\n",
    "\n",
    "- Camada de entrada: dimens√£o igual ao tamanho total do vetor de estado;\n",
    "- Camada oculta 1: 128 neur√¥nios com ativa√ß√£o ReLU;\n",
    "- Camada oculta 2: 64 neur√¥nios com ativa√ß√£o ReLU;\n",
    "- Camada de sa√≠da: 9 neur√¥nios, correspondentes √†s a√ß√µes discretas poss√≠veis (comprar, vender ou manter) para cada um dos tr√™s ativos.\n",
    "\n",
    "A sa√≠da da rede representa o valor esperado de recompensa (Q-value) para cada a√ß√£o, dada a configura√ß√£o atual do ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35f40a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c9bb7",
   "metadata": {},
   "source": [
    "## üåç Ambiente Simulado: `PortfolioEnv`\n",
    "\n",
    "O ambiente `PortfolioEnv` simula o comportamento de um mercado financeiro com tr√™s ativos (VALE3, PETR4, BRFS3), considerando vari√°veis de estado, posi√ß√µes mantidas pelo agente e saldo de caixa dispon√≠vel.\n",
    "\n",
    "A interface segue o padr√£o `gym.Env`, com os principais m√©todos:\n",
    "\n",
    "- `reset()`: reinicializa o ambiente para o primeiro dia √∫til, zera posi√ß√µes e caixa, e retorna o primeiro estado observ√°vel;\n",
    "- `step(action_vector)`: executa as a√ß√µes sobre os ativos, atualiza posi√ß√µes, saldo e calcula a recompensa;\n",
    "- `render()`: imprime informa√ß√µes do estado atual da carteira ‚Äî √∫til para fins de depura√ß√£o e visualiza√ß√£o em tempo real durante o treinamento;\n",
    "- Atributos internos controlam a linha temporal, recompensas acumuladas e hist√≥rico da carteira.\n",
    "\n",
    "As a√ß√µes s√£o vetores discretos com tamanho igual ao n√∫mero de ativos. Para cada ativo, a a√ß√£o pode ser:\n",
    "\n",
    "- `0`: manter posi√ß√£o;\n",
    "- `1`: comprar uma unidade (caso haja saldo);\n",
    "- `2`: vender uma unidade (caso haja posi√ß√£o).\n",
    "\n",
    "A recompensa √© definida com base na varia√ß√£o do valor total da carteira entre `t` e `t+1`, incluindo caixa e valor de mercado das posi√ß√µes. Custos de transa√ß√£o podem ser incorporados como penalidade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e467185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, df, initial_cash=1.0):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "\n",
    "        # DataFrame com os dados de mercado hist√≥ricos (j√° preparados externamente)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "        self.n_assets = 3  # N√∫mero de ativos: VALE3, PETR4, BRFS3\n",
    "        self.initial_cash = initial_cash  # Valor inicial em caixa\n",
    "\n",
    "        # Espa√ßo de a√ß√µes: vetor com 3 entradas (uma por ativo), onde:\n",
    "        # 0 = manter, 1 = comprar, 2 = vender\n",
    "        self.action_space = spaces.MultiDiscrete([3] * self.n_assets)\n",
    "\n",
    "        # Inicializa√ß√µes\n",
    "        self.current_step = 0           # √çndice temporal do ambiente\n",
    "        self.cash = self.initial_cash   # Saldo dispon√≠vel\n",
    "        self.positions = [0] * self.n_assets  # Quantidade de cada ativo na carteira\n",
    "        self.history = []               # Hist√≥rico de recompensas ou estados, se desejado\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reinicia o ambiente para o primeiro dia √∫til da simula√ß√£o\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.positions = [0] * self.n_assets\n",
    "        self.history.clear()\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Aplica a√ß√µes sobre os ativos e avan√ßa para o pr√≥ximo dia.\n",
    "        A l√≥gica detalhada ser√° implementada posteriormente.\n",
    "        \"\"\"\n",
    "        next_state = self._get_state()\n",
    "        reward = 0.0\n",
    "        done = self.current_step >= len(self.df) - 2  # -2 para garantir que t+1 exista\n",
    "        info = {}\n",
    "        self.current_step += 1\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"Retorna o vetor de estado do dia atual, combinando dados de mercado com portf√≥lio\"\"\"\n",
    "        market_data = self.df.iloc[self.current_step].values\n",
    "        return np.concatenate([market_data, self.positions, [self.cash]])\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Exibe o estado atual do ambiente (modo 'human' √© um padr√£o da API gym).\n",
    "        Pode ser ignorado se rodando em scripts automatizados.\n",
    "        \"\"\"\n",
    "        print(f\"Step: {self.current_step} | Caixa: {self.cash:.2f} | Posi√ß√µes: {self.positions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bb90f",
   "metadata": {},
   "source": [
    "## üîÅ L√≥gica de Transi√ß√£o do Ambiente: Implementa√ß√£o do M√©todo `step()`\n",
    "\n",
    "O m√©todo `step()` √© o cora√ß√£o do ambiente `PortfolioEnv`. Ele define como o agente interage com o mercado e como essa intera√ß√£o afeta o estado da carteira. A implementa√ß√£o segue estas etapas:\n",
    "\n",
    "1. **Registro do valor da carteira antes das a√ß√µes**  \n",
    "   Calcula o valor da carteira no tempo `t`, somando o caixa dispon√≠vel e o valor de mercado das posi√ß√µes atuais com os pre√ßos do dia corrente.\n",
    "\n",
    "2. **Execu√ß√£o das a√ß√µes de compra, venda ou manuten√ß√£o**  \n",
    "   Para cada ativo:\n",
    "   - `1` (compra): se houver saldo suficiente, reduz o caixa e incrementa a posi√ß√£o;\n",
    "   - `2` (venda): se houver posi√ß√£o, reduz a quantidade do ativo e adiciona o valor ao caixa;\n",
    "   - `0` (manter): nenhuma altera√ß√£o.\n",
    "\n",
    "   Cada opera√ß√£o de compra ou venda acarreta um **custo de transa√ß√£o fixo**, deduzido do caixa dispon√≠vel. Isso reflete taxas de corretagem, spread e custos operacionais ‚Äî incentivando o agente a evitar opera√ß√µes excessivas.\n",
    "\n",
    "3. **C√°lculo da recompensa**  \n",
    "   Ap√≥s as a√ß√µes, avan√ßa para o pr√≥ximo dia (`t+1`) e calcula o novo valor da carteira.  \n",
    "   A recompensa √© definida como a **varia√ß√£o percentual do valor da carteira** entre `t` e `t+1`.  \n",
    "   Se o epis√≥dio estiver no √∫ltimo dia √∫til (`t+1` n√£o existir), a recompensa √© zero.\n",
    "\n",
    "4. **Atualiza√ß√£o do passo temporal e retorno dos resultados**  \n",
    "   - `next_state`: novo vetor de observa√ß√£o;\n",
    "   - `reward`: feedback para o agente;\n",
    "   - `done`: sinaliza fim do epis√≥dio;\n",
    "   - `info`: dicion√°rio com dados complementares como o valor atual da carteira.\n",
    "\n",
    "Este m√©todo transforma a simula√ß√£o em um ambiente din√¢mico realista, no qual o agente precisa aprender a alocar recursos de forma estrat√©gica e econ√¥mica, ponderando retorno e custo de transa√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cbd5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, action):\n",
    "    \"\"\"\n",
    "    Aplica o vetor de a√ß√µes fornecido (um por ativo), atualiza as posi√ß√µes e o caixa,\n",
    "    avan√ßa o tempo e calcula a recompensa como a varia√ß√£o percentual da carteira l√≠quida.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defini√ß√£o do custo fixo por transa√ß√£o (compra ou venda)\n",
    "    transaction_cost = 0.001  # 0.1% do valor da opera√ß√£o\n",
    "\n",
    "    # Pre√ßos atuais dos ativos (tempo t)\n",
    "    current_prices = self.df.iloc[self.current_step].values\n",
    "\n",
    "    # Valor da carteira antes das a√ß√µes\n",
    "    portfolio_value_before = self.cash + sum([\n",
    "        self.positions[i] * current_prices[i]\n",
    "        for i in range(self.n_assets)\n",
    "    ])\n",
    "\n",
    "    # Aplica√ß√£o das a√ß√µes\n",
    "    for i in range(self.n_assets):\n",
    "        price = current_prices[i]\n",
    "        if action[i] == 1:  # Comprar\n",
    "            total_cost = price * (1 + transaction_cost)\n",
    "            if self.cash >= total_cost:\n",
    "                self.positions[i] += 1\n",
    "                self.cash -= total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26895c9",
   "metadata": {},
   "source": [
    "## ‚úÖ Finaliza√ß√£o do Ambiente Simulado `PortfolioEnv`\n",
    "\n",
    "Com a implementa√ß√£o completa do m√©todo `step()`, o ambiente `PortfolioEnv` est√° agora totalmente funcional e pronto para ser utilizado no treinamento do agente de Reinforcement Learning.\n",
    "\n",
    "Esse ambiente representa, com realismo e controle, um cen√°rio simplificado de mercado no qual o agente:\n",
    "\n",
    "- Observa indicadores t√©cnicos e seu pr√≥prio portf√≥lio (estado);\n",
    "- Decide diariamente se deve comprar, vender ou manter posi√ß√£o (a√ß√£o);\n",
    "- Recebe uma recompensa baseada na evolu√ß√£o da carteira (retorno financeiro ajustado por custo de transa√ß√£o).\n",
    "\n",
    "A inclus√£o de **custos fixos por opera√ß√£o** torna o desafio mais alinhado com o mundo real, penalizando estrat√©gias com excesso de transa√ß√µes e incentivando decis√µes eficientes.\n",
    "\n",
    "Al√©m disso, o ambiente j√° √© compat√≠vel com algoritmos baseados em `gym`, podendo ser diretamente integrado ao loop de treinamento do DQN, Replay Buffer e pol√≠tica de explora√ß√£o Œµ-greedy.\n",
    "\n",
    "A partir deste ponto, seguimos para a configura√ß√£o da estrutura de aprendizado propriamente dita, que inclui:\n",
    "\n",
    "1. Buffer de experi√™ncias (Replay Buffer);\n",
    "2. Pol√≠tica de explora√ß√£o baseada em Œµ-greedy;\n",
    "3. Loop de treinamento com atualiza√ß√£o da rede Q e rede alvo.\n",
    "\n",
    "Com isso, encerramos a Etapa 6.3 ‚Äî *Defini√ß√£o do Ambiente de Simula√ß√£o*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b3200",
   "metadata": {},
   "source": [
    "## üß± Replay Buffer: Armazenamento de Experi√™ncias para Aprendizado\n",
    "\n",
    "O **Replay Buffer** (ou mem√≥ria de experi√™ncia) √© um componente essencial do algoritmo DQN. Ele armazena tuplas de experi√™ncia no formato:\n",
    "\n",
    "(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "Essas experi√™ncias representam intera√ß√µes passadas do agente com o ambiente e s√£o usadas para treinar a rede DQN de forma mais est√°vel e eficiente.\n",
    "\n",
    "### üéØ Objetivos do Replay Buffer\n",
    "\n",
    "- **Descorrelacionar as amostras**: ao usar amostras aleat√≥rias em vez de sequenciais, evitamos instabilidades que ocorrem por dados altamente correlacionados.\n",
    "- **Reutilizar experi√™ncias antigas**: mesmo a√ß√µes passadas que foram ruins (ou boas) podem ser √∫teis para o aprendizado atual.\n",
    "- **Suporte ao aprendizado offline**: a rede aprende a partir do hist√≥rico, n√£o apenas da √∫ltima intera√ß√£o.\n",
    "\n",
    "### ‚öôÔ∏è Estrutura\n",
    "\n",
    "O buffer ser√° implementado como uma estrutura de dados circular (FIFO), com capacidade m√°xima definida (ex: 100.000 intera√ß√µes). Quando o limite √© atingido, os dados mais antigos s√£o descartados automaticamente.\n",
    "\n",
    "A classe `ReplayBuffer` ter√° os seguintes m√©todos principais:\n",
    "\n",
    "- `add(state, action, reward, next_state, done)`: armazena uma nova experi√™ncia;\n",
    "- `sample(batch_size)`: retorna um conjunto aleat√≥rio de experi√™ncias para treino;\n",
    "- `__len__()`: retorna o n√∫mero atual de elementos armazenados.\n",
    "\n",
    "Essa abordagem permite treinar a rede em mini-batches a partir de intera√ß√µes variadas, melhorando a generaliza√ß√£o da pol√≠tica aprendida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b593ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Inicializa o buffer de experi√™ncia.\n",
    "        - capacity: n√∫mero m√°ximo de experi√™ncias armazenadas.\n",
    "        - device: 'cpu' ou 'cuda' para onde os tensores ser√£o enviados.\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)  # estrutura circular FIFO\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Armazena uma nova experi√™ncia no buffer.\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Retorna um mini-batch aleat√≥rio de experi√™ncias do buffer.\n",
    "        Os dados j√° s√£o convertidos para tensores do PyTorch.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convers√£o para tensores PyTorch\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Retorna o n√∫mero atual de experi√™ncias armazenadas.\n",
    "        \"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bf4fa",
   "metadata": {},
   "source": [
    "## üé≤ Pol√≠tica de Explora√ß√£o Œµ-Greedy (com Explota√ß√£o)\n",
    "\n",
    "Durante o treinamento do agente de Reinforcement Learning, √© importante encontrar um equil√≠brio entre:\n",
    "\n",
    "- **Explora√ß√£o** (explore): testar a√ß√µes novas que podem levar a descobertas inesperadas;\n",
    "- **Explota√ß√£o** (exploit): executar a√ß√µes que j√° parecem gerar boas recompensas, com base na pol√≠tica aprendida at√© o momento.\n",
    "\n",
    "A estrat√©gia **Œµ-greedy** regula esse equil√≠brio com uma regra simples:\n",
    "\n",
    "1. Em cada passo de decis√£o, o agente **gera um n√∫mero aleat√≥rio entre 0 e 1**;\n",
    "2. Se esse n√∫mero for **menor que Œµ (epsilon)**, o agente escolhe uma **a√ß√£o aleat√≥ria** (explora√ß√£o);\n",
    "3. Caso contr√°rio, ele escolhe a **melhor a√ß√£o segundo a rede Q atual** (explota√ß√£o da pol√≠tica aprendida).\n",
    "\n",
    "### üîÑ Decaimento de Œµ\n",
    "\n",
    "Para permitir aprendizado eficiente:\n",
    "\n",
    "- Iniciamos com `Œµ = 1.0` (explora√ß√£o m√°xima);\n",
    "- Reduzimos gradualmente at√© um valor m√≠nimo como `Œµ = 0.05`;\n",
    "- Isso garante **ampla explora√ß√£o no in√≠cio** e **explota√ß√£o est√°vel ao final**.\n",
    "\n",
    "Essa pol√≠tica evita que o agente se prenda prematuramente a a√ß√µes sub√≥timas e promove um aprendizado adaptativo ao ambiente.\n",
    "\n",
    "Implementaremos uma fun√ß√£o `select_action()` que aplica a regra Œµ-greedy com suporte a CUDA, retornando vetores de a√ß√£o compat√≠veis com o ambiente `PortfolioEnv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8815765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def select_action(state, q_network, epsilon, n_assets=3):\n",
    "    \"\"\"\n",
    "    Seleciona uma a√ß√£o baseada na pol√≠tica Œµ-greedy.\n",
    "    - Com probabilidade Œµ: retorna a√ß√µes aleat√≥rias (explora√ß√£o);\n",
    "    - Com probabilidade 1 - Œµ: retorna a√ß√µes com maior valor Q estimado (explota√ß√£o).\n",
    "\n",
    "    Par√¢metros:\n",
    "        state (ndarray ou tensor): vetor de estado atual\n",
    "        q_network (nn.Module): rede Q treinada\n",
    "        epsilon (float): taxa de explora√ß√£o\n",
    "        n_assets (int): n√∫mero de ativos (default: 3)\n",
    "\n",
    "    Retorno:\n",
    "        vetor de a√ß√µes discretas (1 por ativo): [0, 2, 1], por exemplo\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explora√ß√£o: a√ß√£o aleat√≥ria para cada ativo\n",
    "        return np.random.randint(0, 3, size=n_assets)\n",
    "\n",
    "    # Explota√ß√£o: a√ß√£o √≥tima via rede Q\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(q_network.net[0].weight.device)\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(state_tensor).cpu().numpy().squeeze()\n",
    "\n",
    "    # A rede retorna um vetor Q de tamanho 9 (3 a√ß√µes x 3 ativos)\n",
    "    # Transformamos isso em uma a√ß√£o por ativo: pega o maior Q por ativo\n",
    "    return np.argmax(q_values.reshape(n_assets, 3), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c6926",
   "metadata": {},
   "source": [
    "## üîÅ Loop de Treinamento do Agente DQN\n",
    "\n",
    "Com todos os componentes implementados (ambiente `PortfolioEnv`, redes DQN, `ReplayBuffer` e pol√≠tica Œµ-greedy), estruturamos agora o ciclo de treinamento do agente com foco em aloca√ß√£o de portf√≥lio.\n",
    "\n",
    "Este loop √© respons√°vel por **coletar experi√™ncias, treinar a rede Q e atualizar a pol√≠tica do agente** ao longo do tempo.\n",
    "\n",
    "### üß† Estrutura Geral\n",
    "\n",
    "1. **Inicializa√ß√£o do ambiente e redes**:\n",
    "   - O agente come√ßa com uma carteira vazia e R$ 100.000,00 em caixa;\n",
    "   - A rede Q (ativa) e a rede Q-alvo (congelada) s√£o inicializadas com pesos iguais.\n",
    "\n",
    "2. **A cada passo de tempo**:\n",
    "   - Seleciona uma a√ß√£o com pol√≠tica Œµ-greedy (balanceando explora√ß√£o e explota√ß√£o);\n",
    "   - Executa a a√ß√£o e observa a recompensa, pr√≥ximo estado e sinal de t√©rmino (`done`);\n",
    "   - Armazena a experi√™ncia no `ReplayBuffer`;\n",
    "   - Quando o buffer cont√©m experi√™ncias suficientes, inicia o processo de aprendizado:\n",
    "     - Amostra um mini-batch aleat√≥rio;\n",
    "     - Calcula o Q-alvo com a rede congelada;\n",
    "     - Calcula o Q-predito com a rede atual;\n",
    "     - Minimiza o erro entre eles com otimizador (loss MSE);\n",
    "   - A cada N passos, atualiza os pesos da rede-alvo.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Hiperpar√¢metros Definidos\n",
    "\n",
    "As seguintes escolhas foram feitas com base em pr√°ticas recomendadas e em testes preliminares com dados financeiros:\n",
    "\n",
    "| Par√¢metro                   | Valor       | Justificativa |\n",
    "|----------------------------|-------------|----------------|\n",
    "| `learning_rate`            | `1e-4`      | Est√°vel em ambientes de baixa dimensionalidade |\n",
    "| `gamma` (fator de desconto)| `0.99`      | Valor alto preserva recompensas futuras |\n",
    "| `batch_size`               | `64`        | Compromisso entre performance e ru√≠do |\n",
    "| `replay_buffer_size`       | `100_000`   | Permite diversidade de experi√™ncias |\n",
    "| `target_update_freq`       | `500` steps | Frequ√™ncia moderada para evitar oscila√ß√£o |\n",
    "| `epsilon_start`            | `1.0`       | Come√ßa com m√°xima explora√ß√£o |\n",
    "| `epsilon_min`              | `0.05`      | Mant√©m um m√≠nimo de explora√ß√£o no fim |\n",
    "| `epsilon_decay`            | `0.995`     | Decaimento gradual a cada epis√≥dio |\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Crit√©rio de Parada: Estabilidade de Desempenho\n",
    "\n",
    "Em vez de treinar por um n√∫mero fixo de epis√≥dios, o agente ser√° treinado at√© atingir **estabilidade em pelo menos duas das seguintes m√©tricas**, avaliadas em janelas m√≥veis:\n",
    "\n",
    "- **Sharpe Ratio**: retorno ajustado pelo risco (ideal acima de 1.0);\n",
    "- **Lucro M√©dio**: lucro acumulado m√©dio por epis√≥dio (positivo e crescente);\n",
    "- **Hit Ratio**: porcentagem de acertos t√°ticos nas decis√µes (ideal acima de 55%).\n",
    "\n",
    "O treinamento ser√° encerrado quando:\n",
    "- As tr√™s m√©tricas estiverem **em faixa aceit√°vel e est√°vel** (com varia√ß√£o padr√£o pequena) por pelo menos **10 epis√≥dios consecutivos**;\n",
    "- Ou quando um **limite m√°ximo de epis√≥dios for alcan√ßado** (ex: 300), evitando overfitting ou sobrecarga.\n",
    "\n",
    "---\n",
    "\n",
    "Essa abordagem permite interromper o treinamento **com base em desempenho real**, promovendo equil√≠brio entre efici√™ncia de capital, risco e precis√£o de decis√£o ‚Äî caracter√≠sticas fundamentais em aplica√ß√µes financeiras reais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85c92593",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Inicializa√ß√£o\u001b[39;00m\n\u001b[1;32m     22\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m q_net \u001b[38;5;241m=\u001b[39m DQN(input_dim\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mreset()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m q_target \u001b[38;5;241m=\u001b[39m DQN(input_dim\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m q_target\u001b[38;5;241m.\u001b[39mload_state_dict(q_net\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Hiperpar√¢metros\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "buffer_capacity = 100_000\n",
    "target_update_freq = 500\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "max_episodes = 300\n",
    "min_buffer_size = 1000\n",
    "\n",
    "# Inicializa√ß√£o\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q_net = DQN(input_dim=env.reset().shape[0], output_dim=9).to(device)\n",
    "q_target = DQN(input_dim=env.reset().shape[0], output_dim=9).to(device)\n",
    "q_target.load_state_dict(q_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "buffer = ReplayBuffer(capacity=buffer_capacity, device=device)\n",
    "\n",
    "# M√©tricas para monitoramento\n",
    "reward_history = []\n",
    "sharpe_window = deque(maxlen=10)\n",
    "hit_window = deque(maxlen=10)\n",
    "\n",
    "step_count = 0\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    rewards = []\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state, q_net, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        rewards.append(reward)\n",
    "        step_count += 1\n",
    "\n",
    "        if len(buffer) > min_buffer_size:\n",
    "            states, actions, rewards_b, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "            # Predi√ß√£o atual\n",
    "            q_values = q_net(states)\n",
    "            actions_flat = actions.view(-1, 1)\n",
    "            q_pred = q_values.gather(1, actions_flat).squeeze()\n",
    "\n",
    "            # Alvo\n",
    "            with torch.no_grad():\n",
    "                q_next = q_target(next_states).max(1)[0]\n",
    "                q_target_vals = rewards_b + (1 - dones) * gamma * q_next\n",
    "\n",
    "            loss = loss_fn(q_pred, q_target_vals)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Atualiza rede alvo\n",
    "            if step_count % target_update_freq == 0:\n",
    "                q_target.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # Atualiza√ß√£o de Œµ\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    # M√©tricas\n",
    "    reward_history.append(total_reward)\n",
    "    sharpe_ratio = np.mean(rewards) / (np.std(rewards) + 1e-6)\n",
    "    hit_ratio = np.mean([r > 0 for r in rewards])\n",
    "\n",
    "    sharpe_window.append(sharpe_ratio)\n",
    "    hit_window.append(hit_ratio)\n",
    "\n",
    "    print(f\"Ep {episode+1} | Recompensa: {total_reward:.3f} | Sharpe: {sharpe_ratio:.2f} | Hit: {hit_ratio:.2%} | Eps: {epsilon:.3f}\")\n",
    "\n",
    "    # Crit√©rio de parada com estabilidade\n",
    "    if len(sharpe_window) == 10:\n",
    "        sharpe_ok = np.mean(sharpe_window) > 1.0 and np.std(sharpe_window) < 0.3\n",
    "        hit_ok = np.mean(hit_window) > 0.55 and np.std(hit_window) < 0.1\n",
    "        lucro_ok = np.mean(reward_history[-10:]) > 0\n",
    "        if sharpe_ok and hit_ok and lucro_ok:\n",
    "            print(\"üèÅ Crit√©rio de estabilidade atingido ‚Äî encerrando o treinamento.\")\n",
    "            break\n",
    "# Gr√°fico de recompensas\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(reward_history, label='Recompensa Total')\n",
    "plt.title('Recompensa Total por Epis√≥dio')\n",
    "plt.xlabel('Epis√≥dio')\n",
    "plt.ylabel('Recompensa')\n",
    "plt.legend()\n",
    "plt.grid()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d38a7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PortfolioEnv.__init__() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Instancia o ambiente e imprime o shape do vetor de estado\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mPortfolioEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m sample_state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimens√£o do vetor de estado:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_state\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: PortfolioEnv.__init__() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "# Instancia o ambiente e imprime o shape do vetor de estado\n",
    "env = PortfolioEnv()\n",
    "sample_state = env.reset()\n",
    "print(\"Dimens√£o do vetor de estado:\", sample_state.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

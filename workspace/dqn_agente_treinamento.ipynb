{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637438ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6f0fcc6",
   "metadata": {},
   "source": [
    "## üéØ Desenvolvimento do Agente DQN para Aloca√ß√£o de Ativos\n",
    "\n",
    "Este notebook implementa a Etapa 6 do projeto de Reinforcement Learning, dedicada √† constru√ß√£o pr√°tica de um agente baseado no algoritmo Deep Q-Network (DQN), aplicado √† aloca√ß√£o din√¢mica de uma carteira com tr√™s ativos: VALE3, PETR4 e BRFS3.\n",
    "\n",
    "O agente ser√° treinado em um ambiente simulado (`PortfolioEnv`), configurado com base nos dados hist√≥ricos processados previamente. A arquitetura ser√° baseada em PyTorch com suporte a CUDA, validando os aprendizados te√≥ricos sobre a fun√ß√£o Q, replay buffer e pol√≠ticas $\\epsilon$-greedy.\n",
    "\n",
    "Etapas implementadas neste notebook:\n",
    "\n",
    "1. Verifica√ß√£o do ambiente e suporte √† GPU;\n",
    "2. Defini√ß√£o da rede neural DQN;\n",
    "3. Constru√ß√£o do ambiente simulado `PortfolioEnv`;\n",
    "4. Configura√ß√£o do buffer de experi√™ncia e par√¢metros de treinamento;\n",
    "5. Execu√ß√£o do treinamento inicial do agente;\n",
    "6. An√°lise dos resultados preliminares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df99822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dispositivo ativo: cuda\n",
      "CUDA dispon√≠vel? True\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas principais para RL e Data Science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Verifica se a GPU est√° dispon√≠vel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"‚úÖ Dispositivo ativo: {device}\")\n",
    "print(f\"CUDA dispon√≠vel? {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a10669",
   "metadata": {},
   "source": [
    "## üß† Arquitetura da Rede Neural DQN\n",
    "\n",
    "A rede neural utilizada neste projeto segue uma arquitetura simples e eficiente, adequada ao tamanho do vetor de estado calculado com base em indicadores t√©cnicos e vari√°veis da carteira.\n",
    "\n",
    "A rede recebe como entrada o vetor de estado do ambiente simulado (`PortfolioEnv`) e retorna os valores Q estimados para cada a√ß√£o poss√≠vel. A estrutura adotada √© composta por:\n",
    "\n",
    "- Camada de entrada: dimens√£o igual ao tamanho total do vetor de estado;\n",
    "- Camada oculta 1: 128 neur√¥nios com ativa√ß√£o ReLU;\n",
    "- Camada oculta 2: 64 neur√¥nios com ativa√ß√£o ReLU;\n",
    "- Camada de sa√≠da: 9 neur√¥nios, correspondentes √†s a√ß√µes discretas poss√≠veis (comprar, vender ou manter) para cada um dos tr√™s ativos.\n",
    "\n",
    "A sa√≠da da rede representa o valor esperado de recompensa (Q-value) para cada a√ß√£o, dada a configura√ß√£o atual do ambiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35f40a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c9bb7",
   "metadata": {},
   "source": [
    "## üåç Ambiente Simulado: `PortfolioEnv`\n",
    "\n",
    "O ambiente `PortfolioEnv` simula o comportamento de um mercado financeiro com tr√™s ativos (VALE3, PETR4, BRFS3), considerando vari√°veis de estado, posi√ß√µes mantidas pelo agente e saldo de caixa dispon√≠vel.\n",
    "\n",
    "A interface segue o padr√£o `gym.Env`, com os principais m√©todos:\n",
    "\n",
    "- `reset()`: reinicializa o ambiente para o primeiro dia √∫til, zera posi√ß√µes e caixa, e retorna o primeiro estado observ√°vel;\n",
    "- `step(action_vector)`: executa as a√ß√µes sobre os ativos, atualiza posi√ß√µes, saldo e calcula a recompensa;\n",
    "- `render()`: imprime informa√ß√µes do estado atual da carteira ‚Äî √∫til para fins de depura√ß√£o e visualiza√ß√£o em tempo real durante o treinamento;\n",
    "- Atributos internos controlam a linha temporal, recompensas acumuladas e hist√≥rico da carteira.\n",
    "\n",
    "As a√ß√µes s√£o vetores discretos com tamanho igual ao n√∫mero de ativos. Para cada ativo, a a√ß√£o pode ser:\n",
    "\n",
    "- `0`: manter posi√ß√£o;\n",
    "- `1`: comprar uma unidade (caso haja saldo);\n",
    "- `2`: vender uma unidade (caso haja posi√ß√£o).\n",
    "\n",
    "A recompensa √© definida com base na varia√ß√£o do valor total da carteira entre `t` e `t+1`, incluindo caixa e valor de mercado das posi√ß√µes. Custos de transa√ß√£o podem ser incorporados como penalidade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7f86f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, df, initial_cash=1.0):\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.n_assets = 3\n",
    "        self.initial_cash = initial_cash\n",
    "\n",
    "        # üö® Novo espa√ßo de a√ß√µes: 3^3 = 27 combina√ß√µes\n",
    "        self.action_space = spaces.Discrete(27)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.positions = [0] * self.n_assets\n",
    "        self.history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.positions = [0] * self.n_assets\n",
    "        self.history.clear()\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Decodifica a a√ß√£o combinada: inteiro [0-26] ‚Üí vetor [a1, a2, a3]\n",
    "        decoded_actions = np.unravel_index(action, (3, 3, 3))\n",
    "\n",
    "        # Pre√ßos atuais dos ativos\n",
    "        current_prices = self.df.iloc[self.current_step].values\n",
    "\n",
    "        # Valor antes das a√ß√µes\n",
    "        portfolio_value_before = self.cash + sum([\n",
    "            self.positions[i] * current_prices[i]\n",
    "            for i in range(self.n_assets)\n",
    "        ])\n",
    "\n",
    "        # Aplica a√ß√µes\n",
    "        transaction_cost = 0.001\n",
    "        for i in range(self.n_assets):\n",
    "            price = current_prices[i]\n",
    "            if decoded_actions[i] == 1:  # Comprar\n",
    "                total_cost = price * (1 + transaction_cost)\n",
    "                if self.cash >= total_cost:\n",
    "                    self.positions[i] += 1\n",
    "                    self.cash -= total_cost\n",
    "            elif decoded_actions[i] == 2:  # Vender\n",
    "                if self.positions[i] > 0:\n",
    "                    total_gain = price * (1 - transaction_cost)\n",
    "                    self.positions[i] -= 1\n",
    "                    self.cash += total_gain\n",
    "\n",
    "        # Avan√ßa o tempo\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "        next_state = self._get_state()\n",
    "\n",
    "        # Valor depois das a√ß√µes\n",
    "        next_prices = self.df.iloc[self.current_step].values\n",
    "        portfolio_value_after = self.cash + sum([\n",
    "            self.positions[i] * next_prices[i]\n",
    "            for i in range(self.n_assets)\n",
    "        ])\n",
    "\n",
    "        reward = portfolio_value_after - portfolio_value_before\n",
    "        info = {\n",
    "            \"valor_portfolio\": portfolio_value_after,\n",
    "            \"lucro\": reward\n",
    "        }\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def _get_state(self):\n",
    "        market_data = self.df.iloc[self.current_step].values\n",
    "        return np.concatenate([market_data, self.positions, [self.cash]])\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step} | Caixa: {self.cash:.2f} | Posi√ß√µes: {self.positions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68bb90f",
   "metadata": {},
   "source": [
    "## üîÅ L√≥gica de Transi√ß√£o do Ambiente: Implementa√ß√£o do M√©todo `step()`\n",
    "\n",
    "O m√©todo `step()` √© o cora√ß√£o do ambiente `PortfolioEnv`. Ele define como o agente interage com o mercado e como essa intera√ß√£o afeta o estado da carteira. A implementa√ß√£o segue estas etapas:\n",
    "\n",
    "1. **Registro do valor da carteira antes das a√ß√µes**  \n",
    "   Calcula o valor da carteira no tempo `t`, somando o caixa dispon√≠vel e o valor de mercado das posi√ß√µes atuais com os pre√ßos do dia corrente.\n",
    "\n",
    "2. **Execu√ß√£o das a√ß√µes de compra, venda ou manuten√ß√£o**  \n",
    "   Para cada ativo:\n",
    "   - `1` (compra): se houver saldo suficiente, reduz o caixa e incrementa a posi√ß√£o;\n",
    "   - `2` (venda): se houver posi√ß√£o, reduz a quantidade do ativo e adiciona o valor ao caixa;\n",
    "   - `0` (manter): nenhuma altera√ß√£o.\n",
    "\n",
    "   Cada opera√ß√£o de compra ou venda acarreta um **custo de transa√ß√£o fixo**, deduzido do caixa dispon√≠vel. Isso reflete taxas de corretagem, spread e custos operacionais ‚Äî incentivando o agente a evitar opera√ß√µes excessivas.\n",
    "\n",
    "3. **C√°lculo da recompensa**  \n",
    "   Ap√≥s as a√ß√µes, avan√ßa para o pr√≥ximo dia (`t+1`) e calcula o novo valor da carteira.  \n",
    "   A recompensa √© definida como a **varia√ß√£o percentual do valor da carteira** entre `t` e `t+1`.  \n",
    "   Se o epis√≥dio estiver no √∫ltimo dia √∫til (`t+1` n√£o existir), a recompensa √© zero.\n",
    "\n",
    "4. **Atualiza√ß√£o do passo temporal e retorno dos resultados**  \n",
    "   - `next_state`: novo vetor de observa√ß√£o;\n",
    "   - `reward`: feedback para o agente;\n",
    "   - `done`: sinaliza fim do epis√≥dio;\n",
    "   - `info`: dicion√°rio com dados complementares como o valor atual da carteira.\n",
    "\n",
    "Este m√©todo transforma a simula√ß√£o em um ambiente din√¢mico realista, no qual o agente precisa aprender a alocar recursos de forma estrat√©gica e econ√¥mica, ponderando retorno e custo de transa√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5cbd5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, action):\n",
    "    \"\"\"\n",
    "    Aplica o vetor de a√ß√µes fornecido (um por ativo), atualiza as posi√ß√µes e o caixa,\n",
    "    avan√ßa o tempo e calcula a recompensa como a varia√ß√£o percentual da carteira l√≠quida.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defini√ß√£o do custo fixo por transa√ß√£o (compra ou venda)\n",
    "    transaction_cost = 0.001  # 0.1% do valor da opera√ß√£o\n",
    "\n",
    "    # Pre√ßos atuais dos ativos (tempo t)\n",
    "    current_prices = self.df.iloc[self.current_step].values\n",
    "\n",
    "    # Valor da carteira antes das a√ß√µes\n",
    "    portfolio_value_before = self.cash + sum([\n",
    "        self.positions[i] * current_prices[i]\n",
    "        for i in range(self.n_assets)\n",
    "    ])\n",
    "\n",
    "    # Aplica√ß√£o das a√ß√µes\n",
    "    for i in range(self.n_assets):\n",
    "        price = current_prices[i]\n",
    "        if action[i] == 1:  # Comprar\n",
    "            total_cost = price * (1 + transaction_cost)\n",
    "            if self.cash >= total_cost:\n",
    "                self.positions[i] += 1\n",
    "                self.cash -= total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26895c9",
   "metadata": {},
   "source": [
    "## ‚úÖ Finaliza√ß√£o do Ambiente Simulado `PortfolioEnv`\n",
    "\n",
    "Com a implementa√ß√£o completa do m√©todo `step()`, o ambiente `PortfolioEnv` est√° agora totalmente funcional e pronto para ser utilizado no treinamento do agente de Reinforcement Learning.\n",
    "\n",
    "Esse ambiente representa, com realismo e controle, um cen√°rio simplificado de mercado no qual o agente:\n",
    "\n",
    "- Observa indicadores t√©cnicos e seu pr√≥prio portf√≥lio (estado);\n",
    "- Decide diariamente se deve comprar, vender ou manter posi√ß√£o (a√ß√£o);\n",
    "- Recebe uma recompensa baseada na evolu√ß√£o da carteira (retorno financeiro ajustado por custo de transa√ß√£o).\n",
    "\n",
    "A inclus√£o de **custos fixos por opera√ß√£o** torna o desafio mais alinhado com o mundo real, penalizando estrat√©gias com excesso de transa√ß√µes e incentivando decis√µes eficientes.\n",
    "\n",
    "Al√©m disso, o ambiente j√° √© compat√≠vel com algoritmos baseados em `gym`, podendo ser diretamente integrado ao loop de treinamento do DQN, Replay Buffer e pol√≠tica de explora√ß√£o Œµ-greedy.\n",
    "\n",
    "A partir deste ponto, seguimos para a configura√ß√£o da estrutura de aprendizado propriamente dita, que inclui:\n",
    "\n",
    "1. Buffer de experi√™ncias (Replay Buffer);\n",
    "2. Pol√≠tica de explora√ß√£o baseada em Œµ-greedy;\n",
    "3. Loop de treinamento com atualiza√ß√£o da rede Q e rede alvo.\n",
    "\n",
    "Com isso, encerramos a Etapa 6.3 ‚Äî *Defini√ß√£o do Ambiente de Simula√ß√£o*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b3200",
   "metadata": {},
   "source": [
    "## üß± Replay Buffer: Armazenamento de Experi√™ncias para Aprendizado\n",
    "\n",
    "O **Replay Buffer** (ou mem√≥ria de experi√™ncia) √© um componente essencial do algoritmo DQN. Ele armazena tuplas de experi√™ncia no formato:\n",
    "\n",
    "(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "Essas experi√™ncias representam intera√ß√µes passadas do agente com o ambiente e s√£o usadas para treinar a rede DQN de forma mais est√°vel e eficiente.\n",
    "\n",
    "### üéØ Objetivos do Replay Buffer\n",
    "\n",
    "- **Descorrelacionar as amostras**: ao usar amostras aleat√≥rias em vez de sequenciais, evitamos instabilidades que ocorrem por dados altamente correlacionados.\n",
    "- **Reutilizar experi√™ncias antigas**: mesmo a√ß√µes passadas que foram ruins (ou boas) podem ser √∫teis para o aprendizado atual.\n",
    "- **Suporte ao aprendizado offline**: a rede aprende a partir do hist√≥rico, n√£o apenas da √∫ltima intera√ß√£o.\n",
    "\n",
    "### ‚öôÔ∏è Estrutura\n",
    "\n",
    "O buffer ser√° implementado como uma estrutura de dados circular (FIFO), com capacidade m√°xima definida (ex: 100.000 intera√ß√µes). Quando o limite √© atingido, os dados mais antigos s√£o descartados automaticamente.\n",
    "\n",
    "A classe `ReplayBuffer` ter√° os seguintes m√©todos principais:\n",
    "\n",
    "- `add(state, action, reward, next_state, done)`: armazena uma nova experi√™ncia;\n",
    "- `sample(batch_size)`: retorna um conjunto aleat√≥rio de experi√™ncias para treino;\n",
    "- `__len__()`: retorna o n√∫mero atual de elementos armazenados.\n",
    "\n",
    "Essa abordagem permite treinar a rede em mini-batches a partir de intera√ß√µes variadas, melhorando a generaliza√ß√£o da pol√≠tica aprendida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f791fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Inicializa o buffer de experi√™ncia.\n",
    "        - capacity: n√∫mero m√°ximo de experi√™ncias armazenadas.\n",
    "        - device: 'cpu' ou 'cuda' para onde os tensores ser√£o enviados.\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque(maxlen=capacity)  # estrutura circular FIFO\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Armazena uma nova experi√™ncia no buffer.\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Retorna um mini-batch aleat√≥rio de experi√™ncias do buffer.\n",
    "        Os dados s√£o convertidos para tensores PyTorch com tipos expl√≠citos.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convers√£o segura para arrays num√©ricos\n",
    "        states = torch.tensor(np.stack([np.array(s, dtype=np.float32) for s in states]), dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(np.stack([np.array(s, dtype=np.float32) for s in next_states]), dtype=torch.float32).to(self.device)\n",
    "\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Retorna o n√∫mero atual de experi√™ncias armazenadas.\n",
    "        \"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bf4fa",
   "metadata": {},
   "source": [
    "## üé≤ Pol√≠tica de Explora√ß√£o Œµ-Greedy (com Explota√ß√£o)\n",
    "\n",
    "Durante o treinamento do agente de Reinforcement Learning, √© importante encontrar um equil√≠brio entre:\n",
    "\n",
    "- **Explora√ß√£o** (explore): testar a√ß√µes novas que podem levar a descobertas inesperadas;\n",
    "- **Explota√ß√£o** (exploit): executar a√ß√µes que j√° parecem gerar boas recompensas, com base na pol√≠tica aprendida at√© o momento.\n",
    "\n",
    "A estrat√©gia **Œµ-greedy** regula esse equil√≠brio com uma regra simples:\n",
    "\n",
    "1. Em cada passo de decis√£o, o agente **gera um n√∫mero aleat√≥rio entre 0 e 1**;\n",
    "2. Se esse n√∫mero for **menor que Œµ (epsilon)**, o agente escolhe uma **a√ß√£o aleat√≥ria** (explora√ß√£o);\n",
    "3. Caso contr√°rio, ele escolhe a **melhor a√ß√£o segundo a rede Q atual** (explota√ß√£o da pol√≠tica aprendida).\n",
    "\n",
    "### üîÑ Decaimento de Œµ\n",
    "\n",
    "Para permitir aprendizado eficiente:\n",
    "\n",
    "- Iniciamos com `Œµ = 1.0` (explora√ß√£o m√°xima);\n",
    "- Reduzimos gradualmente at√© um valor m√≠nimo como `Œµ = 0.05`;\n",
    "- Isso garante **ampla explora√ß√£o no in√≠cio** e **explota√ß√£o est√°vel ao final**.\n",
    "\n",
    "Essa pol√≠tica evita que o agente se prenda prematuramente a a√ß√µes sub√≥timas e promove um aprendizado adaptativo ao ambiente.\n",
    "\n",
    "Implementaremos uma fun√ß√£o `select_action()` que aplica a regra Œµ-greedy com suporte a CUDA, retornando vetores de a√ß√£o compat√≠veis com o ambiente `PortfolioEnv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9c9ef26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def select_action(state, q_net, epsilon):\n",
    "    \"\"\"\n",
    "    Estrat√©gia Œµ-greedy para sele√ß√£o de a√ß√£o combinada (0 a 26).\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Explora√ß√£o: sorteia uma a√ß√£o aleat√≥ria\n",
    "        return np.random.randint(27)\n",
    "    else:\n",
    "        # Explota√ß√£o: escolhe a melhor a√ß√£o via rede Q\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(q_net.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5adb1",
   "metadata": {},
   "source": [
    "## üåê Instancia√ß√£o do Ambiente de Simula√ß√£o\n",
    "\n",
    "Nesta etapa, carregamos o vetor de estado final `vetor_portfolio.csv`, que cont√©m os dados combinados dos tr√™s ativos (VALE3, PETR4, BRFS3) junto com as vari√°veis internas do agente.  \n",
    "Esse vetor representa o ambiente observado pelo agente a cada passo, e ser√° usado para instanciar o `PortfolioEnv`, que simula a din√¢mica do mercado e da carteira de investimentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0a97a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Coluna 'Date' removida do vetor de estado.\n",
      "‚úÖ Ambiente criado com sucesso.\n",
      "üìê Dimens√£o do vetor de estado: (8,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Caminho do vetor final de estados\n",
    "caminho_dados = \"../dados/vetor_portfolio.csv\"\n",
    "df = pd.read_csv(caminho_dados)\n",
    "\n",
    "# Remo√ß√£o da coluna de datas (caso exista)\n",
    "if \"Date\" in df.columns:\n",
    "    df = df.drop(columns=[\"Date\"])\n",
    "    print(\"üßπ Coluna 'Date' removida do vetor de estado.\")\n",
    "\n",
    "# Instancia√ß√£o do ambiente com o vetor corrigido\n",
    "env = PortfolioEnv(df)\n",
    "\n",
    "# Teste de sa√≠da\n",
    "estado_inicial = env.reset()\n",
    "print(\"‚úÖ Ambiente criado com sucesso.\")\n",
    "print(\"üìê Dimens√£o do vetor de estado:\", estado_inicial.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff09de",
   "metadata": {},
   "source": [
    "## üß† Inicializa√ß√£o das Redes DQN e Componentes de Treinamento\n",
    "\n",
    "Nesta etapa, criamos a rede principal `q_net` e a rede alvo `q_target` com base na dimens√£o do vetor de estado obtido do ambiente.  \n",
    "Tamb√©m instanciamos o otimizador `Adam`, a fun√ß√£o de perda `MSELoss` e o `ReplayBuffer`, que ser√° usado para amostragem de transi√ß√µes durante o treinamento do agente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1ef5e6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m estado_inicial\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9\u001b[39m  \u001b[38;5;66;03m# 3 ativos √ó 3 a√ß√µes poss√≠veis: [manter, comprar, vender]\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m q_net \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m q_target \u001b[38;5;241m=\u001b[39m DQN(input_dim\u001b[38;5;241m=\u001b[39minput_dim, output_dim\u001b[38;5;241m=\u001b[39moutput_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m q_target\u001b[38;5;241m.\u001b[39mload_state_dict(q_net\u001b[38;5;241m.\u001b[39mstate_dict())  \u001b[38;5;66;03m# Inicializa a rede alvo com os mesmos pesos\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# üî¢ Hiperpar√¢metros do modelo\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99\n",
    "batch_size = 64\n",
    "buffer_capacity = 100_000\n",
    "target_update_freq = 500\n",
    "\n",
    "# üé≤ Explora√ß√£o\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "max_episodes = 300\n",
    "min_buffer_size = 1000\n",
    "\n",
    "# ‚öôÔ∏è Inicializa√ß√£o do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# üß† Instancia√ß√£o das redes DQN\n",
    "input_dim = estado_inicial.shape[0]\n",
    "output_dim = 9  # 3 ativos √ó 3 a√ß√µes poss√≠veis: [manter, comprar, vender]\n",
    "\n",
    "q_net = DQN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "q_target = DQN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "q_target.load_state_dict(q_net.state_dict())  # Inicializa a rede alvo com os mesmos pesos\n",
    "\n",
    "# üß∞ Otimizador e fun√ß√£o de perda\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# üéí Replay Buffer\n",
    "buffer = ReplayBuffer(capacity=buffer_capacity, device=device)\n",
    "\n",
    "print(\"‚úÖ Redes e componentes de treinamento inicializados com sucesso\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe719c58",
   "metadata": {},
   "source": [
    "## üîÅ Loop de Treinamento do Agente DQN\n",
    "\n",
    "Esta c√©lula executa o treinamento do agente ao longo de v√°rios epis√≥dios.  \n",
    "A cada epis√≥dio, o agente interage com o ambiente, armazena experi√™ncias no `ReplayBuffer`, e atualiza sua rede DQN com base em amostras aleat√≥rias dessas experi√™ncias.  \n",
    "As m√©tricas de desempenho s√£o monitoradas continuamente, e o treinamento √© interrompido se um crit√©rio de estabilidade for atingido (Sharpe Ratio, Hit Ratio e lucro positivo est√°veis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37fcbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# üéØ Treinamento por mini-batch (apenas ap√≥s preenchimento m√≠nimo do buffer)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m>\u001b[39m min_buffer_size:\n\u001b[0;32m---> 29\u001b[0m     states, actions, rewards_b, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Predi√ß√£o atual\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m q_net(states)\n",
      "Cell \u001b[0;32mIn[60], line 32\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m states, actions, rewards, next_states, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Convers√£o segura para arrays num√©ricos\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mstack([np\u001b[38;5;241m.\u001b[39marray(s, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m next_states]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     35\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# üìà Inicializa√ß√£o de buffers de m√©tricas\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "reward_history = []\n",
    "sharpe_window = deque(maxlen=10)\n",
    "hit_window = deque(maxlen=10)\n",
    "\n",
    "step_count = 0\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    rewards = []\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state, q_net, epsilon)  # fun√ß√£o definida previamente\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        rewards.append(reward)\n",
    "        step_count += 1\n",
    "\n",
    "        # üéØ Treinamento por mini-batch (apenas ap√≥s preenchimento m√≠nimo do buffer)\n",
    "        if len(buffer) > min_buffer_size:\n",
    "            states, actions, rewards_b, next_states, dones = buffer.sample(batch_size)\n",
    "\n",
    "            # Predi√ß√£o atual\n",
    "            q_values = q_net(states)\n",
    "            q_pred = q_values.gather(1, actions.view(-1, 1)).squeeze()\n",
    "\n",
    "            # Valor alvo com rede alvo congelada\n",
    "            with torch.no_grad():\n",
    "                q_next = q_target(next_states).max(1)[0]\n",
    "                q_target_vals = rewards_b + (1 - dones) * gamma * q_next\n",
    "\n",
    "            # Otimiza√ß√£o\n",
    "            loss = loss_fn(q_pred, q_target_vals)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Atualiza rede alvo\n",
    "            if step_count % target_update_freq == 0:\n",
    "                q_target.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    # üîÑ Decaimento do epsilon\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
    "\n",
    "    # üìä M√©tricas\n",
    "    reward_history.append(total_reward)\n",
    "    sharpe_ratio = np.mean(rewards) / (np.std(rewards) + 1e-6)\n",
    "    hit_ratio = np.mean([r > 0 for r in rewards])\n",
    "\n",
    "    sharpe_window.append(sharpe_ratio)\n",
    "    hit_window.append(hit_ratio)\n",
    "\n",
    "    print(f\"Ep {episode+1} | Recompensa: {total_reward:.3f} | Sharpe: {sharpe_ratio:.2f} | Hit: {hit_ratio:.2%} | Eps: {epsilon:.3f}\")\n",
    "\n",
    "    # üèÅ Crit√©rio de parada com estabilidade\n",
    "    if len(sharpe_window) == 10:\n",
    "        sharpe_ok = np.mean(sharpe_window) > 1.0 and np.std(sharpe_window) < 0.3\n",
    "        hit_ok = np.mean(hit_window) > 0.55 and np.std(hit_window) < 0.1\n",
    "        lucro_ok = np.mean(reward_history[-10:]) > 0\n",
    "        if sharpe_ok and hit_ok and lucro_ok:\n",
    "            print(\"üèÅ Crit√©rio de estabilidade atingido ‚Äî encerrando o treinamento.\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

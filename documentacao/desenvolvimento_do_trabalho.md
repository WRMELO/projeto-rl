# TRABALHO DA DISCIPLINA DE REINFORCEMENT LEARNING
*WILSON ROBERTO DE MELO - RM 357053*

## 1. Introdu√ß√£o

### 1.1 Contexto do Projeto

Este projeto acad√™mico faz parte da disciplina de Reinforcement Learning (RL), campo da intelig√™ncia artificial em que um agente aprende a tomar decis√µes eficazes atrav√©s da intera√ß√£o constante com o ambiente, buscando maximizar recompensas ao longo do tempo. Especificamente, esta pesquisa est√° focada na aplica√ß√£o de t√©cnicas avan√ßadas de RL para a gest√£o autom√°tica de uma carteira financeira composta por tr√™s importantes a√ß√µes do mercado brasileiro: Vale, Petrobras e Brasil Foods. Esses ativos foram escolhidos devido √† sua relev√¢ncia econ√¥mica e √†s caracter√≠sticas diversificadas de volatilidade e liquidez, o que configura um desafio significativo e realista para o agente de RL.

### 1.2 Motiva√ß√£o e Relev√¢ncia

A motiva√ß√£o central deste projeto √© responder √† crescente necessidade por solu√ß√µes automatizadas de gest√£o financeira que possam auxiliar investidores na tomada de decis√µes r√°pidas e bem fundamentadas diante da complexidade e volatilidade do mercado financeiro atual. Al√©m de atender objetivos acad√™micos, este projeto visa desenvolver um Produto M√≠nimo Vi√°vel (MVP) pr√°tico, que servir√° como ferramenta base para futuras expans√µes e aprimoramentos, potencializando uma gest√£o financeira pessoal mais eficiente e menos sujeita a erros humanos.

### 1.3 Objetivos Gerais e Espec√≠ficos

**Objetivo Geral:**
Desenvolver um agente de Reinforcement Learning (RL) para automatizar a gest√£o de uma carteira de a√ß√µes, maximizando m√©tricas financeiras como lucro acumulado e Sharpe Ratio.

**Objetivos Espec√≠ficos:**

* Definir o ambiente de RL com clareza, especificando estados, a√ß√µes e recompensas;
* Implementar um agente utilizando a t√©cnica de Deep Q-Network (DQN);
* Avaliar o desempenho do agente em simula√ß√µes financeiras detalhadas;
* Gerar insights pr√°ticos para aprimoramento cont√≠nuo da estrat√©gia de investimento desenvolvida.


## 2. Fundamentos Te√≥ricos

### 2.1 O que √© Reinforcement Learning (RL)?

Reinforcement Learning (RL), ou Aprendizado por Refor√ßo, √© um ramo da intelig√™ncia artificial no qual um agente aprende a tomar decis√µes otimizadas atrav√©s da intera√ß√£o cont√≠nua com o ambiente, buscando maximizar a soma das recompensas recebidas ao longo do tempo. O aprendizado ocorre por tentativa e erro, ajustando a√ß√µes futuras com base no retorno das decis√µes passadas.

### 2.2 Aplica√ß√µes de RL em Finan√ßas

No mercado financeiro, o RL √© empregado devido √† sua habilidade em lidar com ambientes din√¢micos e incertos. Contudo, existem outras abordagens de aprendizado com aplica√ß√µes financeiras relevantes:

* **Deep Learning (DL)**: amplamente utilizado em previs√£o de s√©ries temporais e detec√ß√£o de padr√µes complexos em grandes volumes de dados financeiros. Um exemplo pr√°tico √© o uso de redes neurais convolucionais (CNNs) para an√°lise de gr√°ficos de pre√ßos e identifica√ß√£o de padr√µes t√©cnicos.

* **Machine Learning (ML)**: frequentemente usado em classifica√ß√£o de cr√©dito, previs√£o de riscos financeiros e trading algor√≠tmico baseado em aprendizado supervisionado. Um exemplo √© a aplica√ß√£o de algoritmos como XGBoost para prever pre√ßos futuros com base em dados hist√≥ricos.

* **Reinforcement Learning (RL)**: especificamente aplicado em tarefas como gest√£o automatizada de portf√≥lios, execu√ß√£o otimizada de ordens e market making. Institui√ß√µes financeiras, como JP Morgan e Goldman Sachs, j√° utilizam agentes de RL para otimizar suas estrat√©gias de trading e gest√£o de risco em tempo real.

### 2.3 Algoritmos Principais de RL

Os algoritmos mais importantes e utilizados no contexto de Reinforcement Learning incluem:

* **Q-Learning**: algoritmo cl√°ssico de RL baseado em valor, que aprende uma fun√ß√£o Q para estimar a recompensa esperada das a√ß√µes.
* **Deep Q-Network (DQN)**: extens√£o do Q-Learning que usa redes neurais profundas para aproximar a fun√ß√£o Q, permitindo aplica√ß√µes em ambientes complexos e com grandes espa√ßos de estado.

Outros algoritmos importantes:

* **Policy Gradients (PG)**
* **Actor-Critic Methods (A2C, A3C)**
* **Proximal Policy Optimization (PPO)**
* **Deep Deterministic Policy Gradient (DDPG)**

Cada um desses algoritmos tem caracter√≠sticas espec√≠ficas que podem ser aproveitadas conforme o tipo e complexidade do problema abordado.

### 2.4 M√©tricas de Avalia√ß√£o Financeira

Para avaliar modelos financeiros baseados em RL, √© fundamental utilizar m√©tricas adequadas que considerem n√£o s√≥ a rentabilidade, mas tamb√©m o risco. As m√©tricas mais relevantes incluem:

* **Lucro Total**: mede o ganho financeiro acumulado em um per√≠odo determinado. Embora essencial, isoladamente pode n√£o refletir adequadamente o risco assumido.

* **Retorno Anualizado**: padroniza o retorno acumulado para um per√≠odo anual, permitindo compara√ß√µes consistentes entre estrat√©gias distintas.

* **√çndice de Sharpe**: amplamente utilizado em ferramentas de mercado (ex.: QuantConnect, TradingView), mede o retorno excedente (sobre a taxa livre de risco) ajustado pelo risco (volatilidade). √â uma das principais m√©tricas sugeridas devido √† sua efici√™ncia em equilibrar retorno e risco.

* **√çndice de Sortino**: similar ao Sharpe, por√©m considera apenas a volatilidade negativa, penalizando apenas a variabilidade dos retornos abaixo da m√©dia. Muito utilizado por traders que desejam focar na prote√ß√£o contra perdas significativas.

* **M√°ximo Drawdown**: essencial para avaliar o pior cen√°rio poss√≠vel, indica a maior perda acumulada desde um pico anterior at√© um fundo subsequente. √â crucial em estrat√©gias financeiras, especialmente para investidores com baixa toler√¢ncia a risco.

Levando isto em conta, adotamos as seguintes m√©tricas para este projeto: **Lucro Total** devido √† import√¢ncia na mensura√ß√£o do desempenho financeiro bruto; **√çndice de Sharpe** pela sua capacidade de balancear retorno e risco; e **M√°ximo Drawdown** para entender claramente o risco m√°ximo potencial envolvido na estrat√©gia de investimento.


## 3. Defini√ß√£o do Problema de RL no Contexto Financeiro

### 3.1 Estados (Vari√°veis de Mercado)

A defini√ß√£o adequada dos estados √© um dos elementos mais cr√≠ticos no sucesso de qualquer aplica√ß√£o de Reinforcement Learning. No contexto deste projeto, os estados representam o conjunto de informa√ß√µes dispon√≠veis ao agente em cada instante de tempo, com base nas quais as decis√µes de compra, venda ou manuten√ß√£o ser√£o tomadas.

Para garantir uma representa√ß√£o rica, coerente e temporalmente alinhada, adotamos uma **base temporal di√°ria**, com registros para cada dia √∫til de negocia√ß√£o. Essa escolha √© consistente com a frequ√™ncia de disponibiliza√ß√£o dos dados de mercado e √© adequada ao horizonte de decis√£o de investidores individuais e institucionais.

Os estados ser√£o compostos pelas seguintes vari√°veis:

* **Pre√ßos Hist√≥ricos**: inclui os pre√ßos de abertura, m√°ximo, m√≠nimo e fechamento (OHLC) de cada ativo. Esses dados formam a base bruta sobre a qual os demais indicadores s√£o calculados.

* **Volume de Negocia√ß√µes**: quantidade de a√ß√µes negociadas diariamente, fornecendo uma dimens√£o de liquidez e interesse do mercado pelo ativo.

* **Indicadores T√©cnicos** (calculados com base em janelas m√≥veis di√°rias):

  1. **M√©dia M√≥vel Exponencial de 14 dias (EMA-14)**: suaviza os pre√ßos recentes, dando mais peso √†s observa√ß√µes mais novas. Ajuda a detectar tend√™ncias de curto prazo.

  2. **√çndice de For√ßa Relativa (RSI-14)**: mede a velocidade e a mudan√ßa dos movimentos de pre√ßo. Valores extremos sugerem condi√ß√µes de sobrecompra ou sobrevenda.

  3. **MACD (Moving Average Convergence Divergence)**: indicador de momentum que mostra a rela√ß√£o entre duas m√©dias m√≥veis de pre√ßos. Muito utilizado para detectar revers√µes de tend√™ncia.

  4. **Bandas de Bollinger (20 dias)**: representam zonas de suporte e resist√™ncia com base na volatilidade. Indicadores √∫teis para detectar rompimentos ou consolida√ß√µes de pre√ßo.

  5. **Oscilador Estoc√°stico (Stochastic Oscillator - 14,3)**: compara o pre√ßo de fechamento com a faixa de pre√ßos durante um per√≠odo. Ajuda a identificar condi√ß√µes de sobrecompra/sobrevenda com mais sensibilidade que o RSI.

Esses cinco indicadores foram selecionados por sua complementaridade, robustez estat√≠stica e ampla aceita√ß√£o em an√°lises t√©cnicas profissionais. Eles fornecem ao agente uma base informacional equilibrada entre tend√™ncia, momentum e volatilidade.

Todos os dados utilizados ser√£o previamente normalizados e alinhados temporalmente para garantir a consist√™ncia na entrada do modelo de RL.

### 3.2 A√ß√µes (Decis√µes de Investimento)

O conjunto de a√ß√µes poss√≠veis que o agente pode executar a cada passo de tempo foi definido como:

* **Compra (Buy)**: aquisi√ß√£o de uma unidade do ativo, desde que haja capital dispon√≠vel.
* **Venda (Sell)**: venda de uma unidade do ativo, desde que o ativo esteja presente em carteira.
* **Manuten√ß√£o (Hold)**: nenhuma opera√ß√£o, mantendo a posi√ß√£o atual.

Essas tr√™s a√ß√µes formam o espa√ßo de decis√£o b√°sico e suficiente para simular estrat√©gias reais de negocia√ß√£o.

### 3.3 Recompensas (Feedback do Ambiente)

A fun√ß√£o de recompensa √© projetada para guiar o aprendizado do agente com base em seus resultados operacionais. Consideramos tr√™s componentes principais:

* **Lucro L√≠quido Di√°rio**: varia√ß√£o do valor da carteira com base na opera√ß√£o realizada. Essa √© a principal fonte de recompensa positiva.
* **Penalidade por Perda**: caso a opera√ß√£o resulte em preju√≠zo, uma penaliza√ß√£o proporcional √© aplicada.
* **Custos de Transa√ß√£o**: cada compra ou venda incorre em uma penalidade fixa que simula custos operacionais reais (como taxas de corretagem e spread).

Levando esses elementos em considera√ß√£o, estruturamos a fun√ß√£o de recompensa para estimular comportamentos consistentes com uma gest√£o financeira racional: **maximiza√ß√£o de retorno ajustado ao risco** e **minimiza√ß√£o de opera√ß√µes ineficazes**.


## 4. Coleta e An√°lise Inicial de Dados

### 4.1 Fontes dos Dados

Para este projeto, optamos pela utiliza√ß√£o da biblioteca `yfinance` como interface de acesso aos dados hist√≥ricos do Yahoo Finance, uma fonte amplamente reconhecida e gratuita. Os dados foram coletados com frequ√™ncia **di√°ria**, cobrindo os **√∫ltimos cinco anos** de negocia√ß√£o para cada ativo.

O script Python utilizado para a coleta √© apresentado abaixo:

```python
import yfinance as yf
import pandas as pd

# Definir os tickers das a√ß√µes e o per√≠odo
lista_tickers = ['VALE3.SA', 'PETR4.SA', 'BRFS3.SA']  # Vale, Petrobr√°s, Brasil Foods
periodo = '5y'  # √öltimos 5 anos

# Baixar os dados de todos os tickers
for ticker in lista_tickers:
    dados = yf.download(ticker, period=periodo, interval='1d')
    caminho = rf'c:\Users\wilso\OneDrive\MBA FIAP\10. Reinforcement Learning\ENTREGA DO EXERC√çCIO\dados_{ticker.replace(",", "").replace(".SA", "").lower()}.csv'
    dados.to_csv(caminho)
    print(f'Dados de {ticker} salvos em: {caminho}')

# Exibir as primeiras linhas
dados.head()
```

Essa abordagem permitiu extrair diretamente os seguintes campos: `Open`, `High`, `Low`, `Close`, `Adj Close` e `Volume`, que ser√£o posteriormente utilizados para o c√°lculo dos indicadores t√©cnicos.

---

### 4.2 Descri√ß√£o dos Ativos: Vale, Petrobras, Brasil Foods

**VALE3.SA (Vale S.A.)**
A Vale √© uma das maiores mineradoras do mundo, com destaque na produ√ß√£o de min√©rio de ferro e n√≠quel. Sua a√ß√£o possui alta liquidez e volatilidade, sendo fortemente influenciada por fatores internacionais, como demanda da China e pre√ßos globais de commodities.

**PETR4.SA (Petr√≥leo Brasileiro S.A. - Petrobras)**
A Petrobras √© uma das maiores empresas de energia da Am√©rica Latina, com atividades centradas na explora√ß√£o e refino de petr√≥leo. A a√ß√£o preferencial PETR4 apresenta ampla liquidez e sensibilidade a oscila√ß√µes do pre√ßo do barril de petr√≥leo e decis√µes pol√≠tico-econ√¥micas.

**BRFS3.SA (BRF S.A. - Brasil Foods)**
Empresa do setor de alimentos processados, dona de marcas como Sadia e Perdig√£o. Sua a√ß√£o tende a apresentar menor volatilidade relativa em compara√ß√£o com empresas de energia e minera√ß√£o, refletindo caracter√≠sticas t√≠picas do setor de consumo n√£o dur√°vel.

---

### 4.3 An√°lise Explorat√≥ria dos Dados

Ap√≥s a coleta dos dados, uma an√°lise explorat√≥ria foi conduzida para:

* Verificar **consist√™ncia temporal** dos dados (datas coincidentes entre os ativos);
* Confirmar aus√™ncia de **valores nulos ou inconsistentes** nas colunas essenciais (`Close`, `Volume`, etc.);
* Observar a **distribui√ß√£o e comportamento dos pre√ßos** ao longo do tempo, buscando entender padr√µes visuais e outliers;
* Validar a **presen√ßa de varia√ß√£o suficiente** nos dados para o c√°lculo de indicadores t√©cnicos como RSI, MACD e Bandas de Bollinger.

As primeiras linhas de cada dataset foram inspecionadas manualmente e os gr√°ficos de linhas para o `Close` de cada a√ß√£o foram plotados, fornecendo uma vis√£o inicial da din√¢mica de pre√ßos no per√≠odo analisado.

Essa etapa assegura que os dados est√£o prontos para serem normalizados e preparados para entrada no ambiente de aprendizado por refor√ßo.

---

### 4.4 Discuss√£o sobre Escolhas e Integridade dos Dados

**Fonte escolhida:** o Yahoo Finance, acessado via `yfinance`, foi selecionado por sua confiabilidade, cobertura gratuita e facilidade de automa√ß√£o via Python. Evita-se assim depend√™ncia de dados pagos ou formatos propriet√°rios.

**Frequ√™ncia di√°ria:** foi adotada como base por representar um equil√≠brio entre granularidade e ru√≠do. Permite capturar movimentos relevantes do mercado sem sobrecarregar o agente com microvaria√ß√µes intradi√°rias.

**Per√≠odo de 5 anos:** essa escolha cobre diferentes ciclos econ√¥micos, garante amostragem estat√≠stica adequada para c√°lculo de indicadores e fornece volume de dados suficiente para o treinamento e valida√ß√£o do agente de RL.

**Ativos diversificados:** os tr√™s ativos escolhidos representam setores distintos (minera√ß√£o, energia e alimentos), o que enriquece o problema e permite ao agente aprender comportamentos diversos de mercado.

**Sincronia temporal:** a verifica√ß√£o manual e gr√°fica assegurou que os tr√™s datasets compartilham as mesmas datas √∫teis, evitando desalinhamentos que comprometeriam o treinamento.

**Aus√™ncia de dados faltantes:** n√£o foram observadas inconsist√™ncias nas colunas cr√≠ticas. Em caso de valores nulos residuais, ser√° adotado o preenchimento por propaga√ß√£o direta (forward fill).

**Viabilidade dos indicadores t√©cnicos:** a s√©rie temporal de 5 anos com frequ√™ncia di√°ria fornece janelas suficientes para o c√°lculo confi√°vel dos cinco indicadores selecionados (EMA, RSI, MACD, Bollinger Bands, Stochastic Oscillator), sem necessidade de truncamento excessivo.

Com esses pontos atendidos, os dados est√£o validados e prontos para a pr√≥xima etapa de transforma√ß√£o e modelagem.

## 5. Prepara√ß√£o dos Dados para Reinforcement Learning (RL)

A prepara√ß√£o dos dados representa uma das etapas mais importantes de todo o projeto, pois define como o agente perceber√° o ambiente com o qual ir√° interagir. Diferente de modelos supervisionados ‚Äî nos quais cada linha representa um exemplo independente com entrada e sa√≠da ‚Äî no contexto de RL os dados devem ser organizados como uma **sequ√™ncia temporal de decis√µes**, com base em **estados**, **a√ß√µes** e **recompensas**.

Nesta fase, nosso objetivo √© transformar os dados financeiros brutos em um **ambiente observ√°vel e estruturado**, onde cada instante do tempo representa um cen√°rio completo para a tomada de decis√£o do agente.

---

### 5.1 Limpeza e Tratamento dos Dados

Antes de extrair qualquer tipo de informa√ß√£o √∫til para o aprendizado, √© necess√°rio garantir que os dados estejam √≠ntegros, consistentes e sincronizados entre os diferentes ativos.

As principais a√ß√µes realizadas nesta etapa s√£o:

- **Remo√ß√£o ou interpola√ß√£o de valores nulos** em colunas essenciais como `Close`, `Open` e `Volume`, que comprometem o c√°lculo de indicadores t√©cnicos e a simula√ß√£o de transa√ß√µes;
- **Alinhamento de datas entre os tr√™s ativos (VALE3, PETR4 e BRFS3)**, tomando como base a interse√ß√£o entre os dias v√°lidos de todos os ativos para garantir consist√™ncia temporal;
- **Convers√£o e padroniza√ß√£o de tipos de dados**, assegurando que campos como `Volume` estejam devidamente formatados como `float` ou `int`, conforme apropriado;
- **Indexa√ß√£o temporal unificada**, utilizando a coluna `Date` como √≠ndice de linha, o que facilita opera√ß√µes como janelas m√≥veis, compara√ß√µes entre ativos e replica√ß√£o da realidade sequencial do mercado.

Esse processo assegura que, a cada dia de negocia√ß√£o, o agente receba uma vis√£o completa, confi√°vel e compar√°vel entre todos os ativos envolvidos.

---

### 5.2 Engenharia de Features (Indicadores T√©cnicos)

Com os dados devidamente tratados, passamos √† extra√ß√£o de vari√°veis derivadas que representam o comportamento do mercado de forma mais informativa e sint√©tica. Essa etapa, conhecida como engenharia de features, visa aumentar o poder explicativo das entradas fornecidas ao agente de Reinforcement Learning.

Optou-se por utilizar **indicadores t√©cnicos consagrados na literatura financeira**, aplicados individualmente aos ativos VALE3, PETR4 e BRFS3, com foco na simplicidade, complementaridade e efici√™ncia computacional. O objetivo n√£o foi a exaustividade, mas sim a constru√ß√£o de um **vetor de estado enxuto e representativo**, adequado ao treinamento de agentes com capacidade generalizadora.

Os indicadores selecionados para compor o Produto M√≠nimo Vi√°vel (MVP) foram:

- **SMA-5 e SMA-20 (M√©dias M√≥veis Simples)**: suavizam flutua√ß√µes di√°rias e capturam tend√™ncias de curto e m√©dio prazo.
- **Retorno Di√°rio (`Return`)**: varia√ß√£o percentual do pre√ßo de fechamento, usado para detectar revers√µes e movimentos recentes.
- **Volatilidade (`Volatility`)**: desvio padr√£o do retorno di√°rio em janela de 10 dias, utilizado como proxy de risco.
- **RSI-14 (√çndice de For√ßa Relativa)**: indicador de momentum que identifica condi√ß√µes de sobrecompra ou sobrevenda.
- **Volume (transformado com log1p)**: expressa a intensidade da atividade de negocia√ß√£o em escala menos sens√≠vel a picos extremos.

Essas vari√°veis foram escolhidas por sua complementaridade: enquanto algumas expressam **dire√ß√£o ou for√ßa de tend√™ncia** (como SMA e RSI), outras representam **risco, varia√ß√£o recente e liquidez** ‚Äî fornecendo ao agente m√∫ltiplas dimens√µes informacionais para a tomada de decis√£o.

---

#### Normaliza√ß√£o

A fim de garantir estabilidade e efici√™ncia no processo de aprendizado, todas as vari√°veis foram **normalizadas por ativo** utilizando abordagens adequadas ao tipo de dado:

- **Z-score**: aplicado a `Return`, `Volatility` e `RSI`, centralizando os dados em m√©dia zero com vari√¢ncia unit√°ria;
- **Escala Relativa**: aplicada aos pre√ßos (`Close`, `SMA`), dividindo-os pelo valor da primeira observa√ß√£o v√°lida;
- **Logaritmiza√ß√£o**: aplicada ao `Volume`, com uso de `log1p` para conter a escala e preservar distribui√ß√£o.

Os vetores de estado resultantes foram salvos como arquivos `.csv` separados para cada ativo (`VALE3_estado.csv`, etc.), contendo as colunas finalizadas e prontas para compor o ambiente de simula√ß√£o.

Indicadores adicionais como `MACD`, `Bandas de Bollinger`, `EMA` ou `OBV` foram considerados durante o planejamento, mas foram exclu√≠dos da vers√£o inicial do MVP para manter a estrutura leve, interpret√°vel e focada no desempenho essencial. Esses indicadores permanecem documentados como **poss√≠veis extens√µes futuras** do modelo.

---

### 5.3 Defini√ß√£o do Espa√ßo de Estados para RL

Com os indicadores t√©cnicos j√° calculados e normalizados para os tr√™s ativos selecionados, o pr√≥ximo passo √© estruturar o vetor de estado que ser√° utilizado como entrada pelo agente de Reinforcement Learning.

Diferentemente de abordagens supervisionadas tradicionais, em que as vari√°veis independentes (features) se referem unicamente ao ambiente, em RL √© necess√°rio incorporar tamb√©m o **estado interno do agente** ‚Äî ou seja, seu portf√≥lio atual e recursos dispon√≠veis.

Dessa forma, o vetor de estado ser√° composto por duas partes:

- Um conjunto de vari√°veis de mercado, extra√≠das e normalizadas para cada ativo (`Close_Norm`, `SMA5_Norm`, `Return_Z`, `Volume_Log`, etc.);
- Um conjunto de vari√°veis internas ao agente, contendo:
  - As **posi√ß√µes atuais em cada ativo**, codificadas como 0 (fora da posi√ß√£o) ou 1 (posi√ß√£o comprada);
  - O **saldo de caixa dispon√≠vel**, em reais, que ser√° atualizado a cada passo de tempo.

Essa estrutura garante que o agente, ao observar o ambiente, tenha uma vis√£o completa tanto das **condi√ß√µes externas** (pre√ßos, tend√™ncias e liquidez dos ativos) quanto do seu **estado interno** (disponibilidade financeira e aloca√ß√£o atual), condi√ß√£o fundamental para a simula√ß√£o realista de decis√µes de investimento.

A constru√ß√£o do vetor de portf√≥lio ser√° feita utilizando a mesma linha temporal dos vetores de mercado, garantindo a integra√ß√£o completa das informa√ß√µes necess√°rias √† simula√ß√£o. Essa etapa ser√° incorporada √† l√≥gica de inicializa√ß√£o da carteira e simula√ß√£o de decis√µes, apresentada no item seguinte.


---

### 5.4 Inicializa√ß√£o da Carteira e Estrat√©gias de Benchmark

Para simular um ambiente realista, o agente ser√° inicializado no dia **05/05/2020** com uma carteira contendo **R$ 100.000,00 em caixa**. Nenhum ativo ser√° comprado neste momento inicial.

As decis√µes poss√≠veis a cada passo ser√£o:

- `0` = manter a posi√ß√£o atual (n√£o realizar transa√ß√µes);
- `1` = comprar, caso haja saldo dispon√≠vel e nenhuma posi√ß√£o atual naquele ativo;
- `2` = vender, caso haja uma posi√ß√£o vigente no ativo selecionado.

Como n√£o existe uma ‚Äúresposta certa‚Äù a cada decis√£o (como haveria em classifica√ß√£o supervisionada), a avalia√ß√£o do desempenho ser√° feita com base em m√©tricas financeiras como:

- **Lucro acumulado**;
- **Sharpe Ratio** (rela√ß√£o entre retorno e risco);
- **Drawdown m√°ximo** (pior sequ√™ncia de perdas);
- **Volatilidade da carteira**;
- **Turnover de transa√ß√µes** (frequ√™ncia de compra e venda).

Para validar o desempenho, os resultados do agente ser√£o comparados com **estrat√©gias de benchmark** conhecidas:

- **Buy and Hold**: compra de todos os ativos na data inicial, com manuten√ß√£o at√© o final;
- **Varia√ß√£o do IBOVESPA**: como comparador de mercado amplo;
- **Estrat√©gia de cruzamento de m√©dias m√≥veis (SMA-20 vs SMA-50)**: como baseline t√©cnico automatizado.

Al√©m disso, implementaremos a m√©trica de **signal accuracy** (ou *hit ratio*), que avalia a qualidade t√°tica das decis√µes:

- Quando o agente recomenda **comprar** e o pre√ßo sobe nos dias seguintes;
- Quando recomenda **vender** e o pre√ßo cai posteriormente.

Essa taxa ser√° avaliada em tr√™s horizontes distintos: **1, 3 e 5 dias**, permitindo identificar se o agente est√° respondendo bem a oscila√ß√µes de curt√≠ssimo prazo, microtend√™ncias ou movimentos mais sustentados.

---

### 5.5 Divis√£o Temporal dos Dados

Dada a natureza sequencial e autocorrelacionada dos dados financeiros, a divis√£o em treino, valida√ß√£o e teste n√£o pode ser feita de forma aleat√≥ria, como em outras abordagens.

Adotaremos uma **divis√£o cronol√≥gica** baseada em propor√ß√µes temporais:

- **Treinamento**: primeiros 70% dos dados, usados para aprendizado da pol√≠tica;
- **Valida√ß√£o**: 15% seguintes, reservados para ajustes finos e tuning de par√¢metros;
- **Teste**: √∫ltimos 15%, utilizados exclusivamente para simula√ß√£o e avalia√ß√£o final.

Essa separa√ß√£o garante que o agente aprenda apenas com o passado, e que sua performance seja testada em dados que ele nunca viu, refletindo com fidelidade o desafio de investir em um mercado real.

---

## 6. Implementa√ß√£o do Agente de Reinforcement Learning (RL)

A implementa√ß√£o do agente de RL √© o cora√ß√£o do projeto. √â neste ponto que o conhecimento te√≥rico acumulado ‚Äî desde a defini√ß√£o do problema at√© a modelagem dos dados ‚Äî se transforma em uma entidade computacional capaz de interagir com o ambiente financeiro de forma aut√¥noma e inteligente.

Nesta se√ß√£o, detalharemos a constru√ß√£o do agente utilizando a t√©cnica de Deep Q-Network (DQN), abordando sua estrutura de rede, integra√ß√£o com o ambiente de simula√ß√£o, l√≥gica de aprendizado e prepara√ß√£o para o treinamento inicial.

### 6.1 Introdu√ß√£o ao Deep Q-Network (DQN)

O algoritmo Deep Q-Network (DQN) √© uma extens√£o do m√©todo tradicional de Q-Learning, projetado para operar em ambientes com grandes espa√ßos de estados, onde tabelas Q se tornam invi√°veis. Ele substitui a tabela por uma rede neural profunda que aproxima a fun√ß√£o Q, permitindo generaliza√ß√µes e aprendizados mais eficientes.

Em termos pr√°ticos, o DQN busca estimar a fun√ß√£o de valor a√ß√£o-estado:

$$
Q(s, a) \approx \mathbb{E}[R_t \mid s_t = s, a_t = a]
$$

Onde:

- $s$ representa o **estado atual** do ambiente (informa√ß√µes de mercado e da carteira do agente);
- $a$ representa a **a√ß√£o executada** pelo agente (compra, venda ou manuten√ß√£o por ativo);
- $R_t$ √© o **retorno acumulado futuro**, resultado da execu√ß√£o da a√ß√£o $a$ no estado $s$;
- $s'$ √© o **pr√≥ximo estado** do ambiente, resultante da a√ß√£o tomada;
- `done` √© um **indicador booleano** que sinaliza se o epis√≥dio terminou (por exemplo, quando se atinge o final da s√©rie hist√≥rica).

A fun√ß√£o Q √© atualizada com base na Equa√ß√£o de Bellman:

$$
\text{Loss} = \left[ Q(s_t, a_t) - \left( r_t + \gamma \cdot \max_a Q(s_{t+1}, a) \right) \right]^2
$$

Para garantir estabilidade no aprendizado, o DQN adota dois mecanismos fundamentais:

- **Replay Buffer**: armazena transi√ß√µes $(s, a, r, s', done)$, que s√£o amostradas aleatoriamente para evitar correla√ß√µes temporais;
- **Target Network**: uma c√≥pia da rede Q usada apenas para calcular os alvos de treinamento, atualizada periodicamente com os pesos da rede principal.

---

### 6.2 Estrutura da Rede Neural

A arquitetura adotada para a rede DQN neste projeto foi desenhada para balancear expressividade e efici√™ncia computacional. Como entrada, a rede recebe um vetor com:

- Indicadores t√©cnicos dos ativos (normalizados);
- Posi√ß√µes atuais em cada ativo;
- Saldo de caixa dispon√≠vel.

A estrutura da rede √© composta por:

- **Camada de Entrada**: dimens√£o igual ao vetor de estado;
- **Camadas Ocultas**:
  - `Dense(128)` com fun√ß√£o de ativa√ß√£o ReLU;
  - `Dense(64)` com fun√ß√£o de ativa√ß√£o ReLU;
- **Camada de Sa√≠da**:
  - `Dense(n_actions)` com ativa√ß√£o linear, retornando os valores estimados $Q(s, a)$ para cada a√ß√£o.

A rede principal √© treinada com erro quadr√°tico m√©dio (MSE) entre os valores previstos e os alvos calculados com a rede-alvo, conforme a equa√ß√£o de Bellman.

---

### 6.3 Defini√ß√£o do Ambiente de Simula√ß√£o

O ambiente de simula√ß√£o √© respons√°vel por modelar o mercado financeiro e a carteira do agente. Ele foi estruturado para seguir a interface `gym.Env` da OpenAI, com os seguintes m√©todos principais:

- `reset()`: reinicializa o ambiente com R$100.000 em caixa (normalizado como 1.0), posi√ß√µes zeradas e retorna o estado inicial;
- `step(action)`: executa a a√ß√£o informada, atualiza as posi√ß√µes e o caixa, calcula a recompensa e retorna:
  - o novo estado;
  - a recompensa recebida;
  - o sinal de t√©rmino do epis√≥dio (`done`);
  - informa√ß√µes adicionais para an√°lise (`info`).

As a√ß√µes s√£o vetores discretos por ativo:

- `0` = manter;
- `1` = comprar (caso haja caixa);
- `2` = vender (caso haja posi√ß√£o).

A recompensa √© baseada na varia√ß√£o do patrim√¥nio l√≠quido (caixa + posi√ß√µes), penalizada por custos de transa√ß√£o e preju√≠zos. O epis√≥dio termina ao alcan√ßar o fim da s√©rie hist√≥rica.

---

### 6.4 Treinamento Inicial do Agente

Com o ambiente e a rede definidos, o treinamento do agente ocorre por meio da intera√ß√£o repetida com o ambiente, ajustando os pesos da rede para melhorar suas decis√µes futuras. O processo segue os seguintes passos:

1. **Inicializa√ß√£o do Replay Buffer** com capacidade para 10.000 transi√ß√µes;
2. **Execu√ß√£o de m√∫ltiplos epis√≥dios**, onde:
   - O agente observa o estado atual;
   - Escolhe uma a√ß√£o pela pol√≠tica $\epsilon$-greedy;
   - Executa a a√ß√£o, obt√©m a recompensa e o novo estado;
   - Armazena a transi√ß√£o no buffer;
   - Amostra minibatches aleat√≥rios para treinar a rede;
3. **Atualiza√ß√£o peri√≥dica da target network** a cada $C$ itera√ß√µes;
4. **Monitoramento por valida√ß√£o** para evitar overfitting e acompanhar m√©tricas de performance.

O valor de $\epsilon$ inicia alto (ex: 1.0) e decresce progressivamente at√© um valor m√≠nimo (ex: 0.1), permitindo ao agente alternar entre **explora√ß√£o** (tomar decis√µes aleat√≥rias para descobrir novas possibilidades) e **explora√ß√£o do conhecimento aprendido** (agir com base na pol√≠tica j√° treinada).

O treinamento ser√° conduzido por 200 epis√≥dios, com registro das seguintes m√©tricas:

- Lucro acumulado;
- Sharpe Ratio;
- Drawdown m√°ximo;
- Acur√°cia dos sinais (hit ratio em 1, 3 e 5 dias).

Os melhores modelos ser√£o salvos para posterior avalia√ß√£o em ambiente de teste.


5. **Um coment√°rio importante:** Por que Utilizar Replay Buffer com Dados Sequenciais?

Embora os dados de mercado sejam naturalmente sequenciais ‚Äî ou seja, cada dia depende do anterior ‚Äî o uso do **Replay Buffer** no treinamento do agente DQN √© **fundamental para garantir a estabilidade e efici√™ncia do aprendizado**.

### O Problema da Correla√ß√£o Temporal

Se trein√°ssemos o agente diretamente com sequ√™ncias cronol√≥gicas (ex: dias consecutivos: t, t+1, t+2...), o modelo sofreria com:

- **Overfitting local**: aprendizado baseado apenas em padr√µes recentes, sem capacidade de generaliza√ß√£o;
- **Instabilidade na otimiza√ß√£o**: mudan√ßas abruptas de gradiente, pois os dados s√£o altamente autocorrelacionados;
- **Baixa diversidade nos exemplos**: o agente teria poucas situa√ß√µes distintas para comparar e aprender.

###  A Solu√ß√£o: Replay Buffer

O **Replay Buffer** resolve esse problema ao permitir que a rede aprenda com **mini-batches aleat√≥rios de experi√™ncias passadas**, quebrando a sequ√™ncia e promovendo generaliza√ß√£o.

- As experi√™ncias s√£o **coletadas em ordem temporal real**, respeitando a sequ√™ncia do ambiente;
- Mas s√£o **usadas de forma embaralhada durante o treino**, o que reduz a correla√ß√£o entre amostras.

### Analogia Real

Imagine um trader experiente que, ao tomar uma decis√£o hoje, **lembra de diversos eventos anteriores no mercado**, mesmo que tenham ocorrido meses atr√°s.  
O Replay Buffer simula essa capacidade: o agente **aprende com base em uma mem√≥ria diversificada de situa√ß√µes**, n√£o apenas no que acabou de acontecer.

### Benef√≠cios

- Reduz vari√¢ncia nos gradientes e melhora a estabilidade do DQN;
- Permite revisitar experi√™ncias raras ou valiosas no treinamento;
- Promove maior robustez da pol√≠tica aprendida frente a novos dados.

Portanto, mesmo que o ambiente seja sequencial por natureza, o uso do Replay Buffer **√© n√£o s√≥ compat√≠vel, mas essencial** para um aprendizado mais est√°vel e eficaz em Reinforcement Learning.

---


## 7. Avalia√ß√£o e Otimiza√ß√£o do Agente

### 7.1 Simula√ß√£o do Desempenho do Agente

Com o agente treinado ao longo de 300 epis√≥dios utilizando a t√©cnica de Deep Q-Network (DQN), procedeu-se √† avalia√ß√£o de sua pol√≠tica em dados at√© ent√£o n√£o vistos ‚Äî mais especificamente, o conjunto de valida√ß√£o correspondente aos 15% centrais da s√©rie temporal total. Esta etapa foi conduzida com Œµ = 0.0, ou seja, **o agente atuou exclusivamente com base na pol√≠tica aprendida, sem qualquer explora√ß√£o aleat√≥ria**.

O ambiente foi reinicializado com os dados de valida√ß√£o, mantendo as mesmas condi√ß√µes iniciais da carteira: **R$ 100.000,00 em caixa e nenhuma posi√ß√£o comprada**. A simula√ß√£o percorreu diariamente o vetor de valida√ß√£o, permitindo ao agente decidir entre manter, comprar ou vender em cada um dos tr√™s ativos: VALE3, PETR4 e BRFS3.

Durante essa simula√ß√£o, a cada passo, registrou-se o valor total da carteira (caixa + valor das posi√ß√µes a pre√ßo de mercado), compondo a s√©rie patrimonial do agente. Essa curva de valor acumulado foi usada como base para o c√°lculo das m√©tricas financeiras descritas a seguir.

---

### 7.2 Avalia√ß√£o usando M√©tricas Financeiras

A avalia√ß√£o da performance do agente no conjunto de valida√ß√£o foi conduzida com base nas tr√™s m√©tricas previamente definidas na se√ß√£o 2.4:

- **Lucro Total**
- **√çndice de Sharpe**
- **M√°ximo Drawdown**

Os resultados observados foram os seguintes:

- üìà **Lucro Total:** R$ 6,78  
- üìä **√çndice de Sharpe:** 0,06  
- üìâ **M√°ximo Drawdown:** R$ 5,33 (0,01%)

**Lucro Total:**  
Representa a diferen√ßa absoluta entre o valor final e inicial da carteira ao longo da simula√ß√£o (~183 dias √∫teis). Com um valor inicial de R$ 100.000,00, o agente terminou o per√≠odo com R$ 100.006,78. Esse resultado indica que o agente **preservou o capital**, mas **n√£o gerou retorno relevante** em termos absolutos (0,0068% no per√≠odo completo).

**√çndice de Sharpe:**  
Calculado como a m√©dia dos retornos di√°rios da carteira dividida pelo seu desvio padr√£o, considerando taxa livre de risco nula. O valor de 0,06 evidencia que o **retorno m√©dio foi muito pr√≥ximo de zero em rela√ß√£o ao risco** assumido, o que caracteriza uma pol√≠tica neutra em termos de performance ajustada √† volatilidade.

**M√°ximo Drawdown:**  
Foi medido como a maior perda acumulada entre um pico e o fundo subsequente da curva patrimonial. O valor observado de R$ 5,33 ‚Äî equivalente a 0,01% ‚Äî sugere que **o agente operou de maneira extremamente conservadora**, evitando grandes flutua√ß√µes negativas, mas tamb√©m sem assumir posi√ß√µes que maximizassem retorno.

---

### 7.3 An√°lise de Resultados

A an√°lise combinada das m√©tricas e da curva de evolu√ß√£o do patrim√¥nio revela que, apesar de estruturalmente est√°vel, **a pol√≠tica aprendida pelo agente se mostrou excessivamente passiva**. Em termos pr√°ticos, o agente adotou um comportamento semelhante ao de manter caixa, realizando poucas opera√ß√µes com efeito marginal sobre o valor da carteira.

Isso pode ter sido reflexo de v√°rios fatores interligados:

1. **Fun√ß√£o de recompensa muito conservadora**, que premia varia√ß√µes positivas mas n√£o incentiva suficientemente a tomada de risco controlado;
2. **Espa√ßo de a√ß√£o restrito**, com apenas uma unidade transacion√°vel por ativo e sem gest√£o de quantidade;
3. **Aus√™ncia de penaliza√ß√µes por inatividade**, o que permite que o agente "aprenda" que n√£o fazer nada pode ser suficiente para evitar preju√≠zo ‚Äî mesmo que √† custa de oportunidades de ganho;
4. **Tempo de treinamento ainda limitado**, o que pode restringir a capacidade de generaliza√ß√£o e descoberta de padr√µes mais sofisticados.

Apesar disso, o resultado n√£o √© desprez√≠vel: o agente conseguiu preservar o capital de forma consistente, com drawdowns m√≠nimos e sem colapsos de performance. Isso demonstra que a estrutura geral do ambiente de simula√ß√£o, a arquitetura do DQN e a estrat√©gia de treinamento adotadas s√£o vi√°veis ‚Äî embora ainda insuficientes para gerar uma pol√≠tica financeiramente interessante.

Portanto, conclui-se que o agente est√° operando dentro de um **padr√£o t√©cnico aceit√°vel**, mas requer otimiza√ß√µes espec√≠ficas em seus mecanismos de decis√£o e aprendizado para evoluir de uma pol√≠tica neutra para uma pol√≠tica efetivamente lucrativa.

As dire√ß√µes para esse aprimoramento ser√£o discutidas na se√ß√£o seguinte (7.4).

### 7.4 Ajustes e Otimiza√ß√£o do Modelo

Diante dos resultados observados na se√ß√£o anterior ‚Äî especialmente a neutralidade da pol√≠tica aprendida pelo agente ‚Äî torna-se evidente a necessidade de realizar interven√ß√µes estruturadas com o objetivo de aprimorar o desempenho do modelo de Reinforcement Learning. A seguir, s√£o propostas otimiza√ß√µes agrupadas em tr√™s dimens√µes: ambiente, arquitetura do agente e estrat√©gia de aprendizado.

---

**1. Ajustes no Ambiente de Simula√ß√£o**

* **Introdu√ß√£o de penalidade por inatividade:**  
Atualmente, o agente pode permanecer inerte ao longo de todo o epis√≥dio e ainda preservar o capital, o que n√£o o incentiva a buscar oportunidades de ganho. Recomenda-se introduzir uma **penaliza√ß√£o leve por manuten√ß√£o prolongada de posi√ß√£o nula**, estimulando o agente a tomar decis√µes estrat√©gicas ao inv√©s de se omitir.

* **Inclus√£o de custos de transa√ß√£o vari√°veis:**  
Embora o modelo suporte penaliza√ß√£o por opera√ß√£o, na vers√£o atual ela ainda n√£o foi implementada. A introdu√ß√£o de custos de transa√ß√£o proporcionais ao valor da opera√ß√£o pode simular o impacto real de corretagem, spread e slippage, incentivando o agente a otimizar o volume e o momento das opera√ß√µes.

* **Aumento do espa√ßo de a√ß√£o com m√∫ltiplas unidades:**  
Atualmente o agente s√≥ pode comprar ou vender uma unidade de cada ativo. A amplia√ß√£o para m√∫ltiplos n√≠veis (ex.: comprar 1, 2 ou 3 unidades) ou a introdu√ß√£o de propor√ß√µes pode torn√°-lo mais realista e aumentar a complexidade estrat√©gica da pol√≠tica aprendida.

---

**2. Otimiza√ß√£o da Rede Neural e da Fun√ß√£o de Recompensa**

* **Refinamento da fun√ß√£o de recompensa:**  
A recompensa atual mede a varia√ß√£o absoluta do valor da carteira. Sugere-se testar abordagens que incorporem m√©tricas ajustadas ao risco, como **retorno percentual** ou at√© **incrementos no Sharpe Ratio**, como proxy da qualidade da decis√£o, promovendo maior alinhamento com o objetivo financeiro final.

* **Adi√ß√£o de dropout ou batch normalization:**  
Embora a rede atual seja est√°vel, sua capacidade de generaliza√ß√£o pode ser aumentada com a inclus√£o de mecanismos como **dropout** (para evitar overfitting) ou **batch normalization** (para estabilizar os gradientes). Isso pode permitir o uso de redes ligeiramente mais profundas sem comprometer a estabilidade.

* **Ado√ß√£o de loss functions alternativas:**  
O uso de **Huber Loss** em lugar do MSE pode tornar o aprendizado mais robusto a outliers e flutua√ß√µes abruptas nos retornos di√°rios.

---

**3. Estrat√©gias de Aprendizado e Treinamento**

* **Aumento do n√∫mero de epis√≥dios de treinamento:**  
O agente foi treinado por 300 epis√≥dios, o que √© adequado para um primeiro experimento, mas ainda modesto para uma tarefa com grande espa√ßo de estados e a√ß√µes. Recomenda-se expandir para 1.000 epis√≥dios ou mais, com monitoramento cont√≠nuo das m√©tricas de desempenho.

* **Pol√≠tica de decaimento de Œµ mais lenta e customizada:**  
O decaimento exponencial de Œµ de 1.0 para 0.1 ocorreu de forma relativamente r√°pida, limitando a fase de explora√ß√£o. Uma abordagem de decaimento **linear ou baseada em patamares** (ex.: manter Œµ alto nas primeiras 100 intera√ß√µes) pode favorecer uma pol√≠tica mais informada antes da explota√ß√£o.

* **Treinamento cont√≠nuo com avalia√ß√£o intercalada:**  
Dividir o treinamento em ciclos e intercalar rodadas de avalia√ß√£o com rein√≠cio do ambiente permite identificar com mais clareza pontos de inflex√£o no aprendizado e evita que o modelo ‚Äúesque√ßa‚Äù comportamentos √∫teis por overtraining.

---

Com esses ajustes, espera-se que o agente evolua de uma postura neutra e conservadora para uma pol√≠tica mais sofisticada, responsiva ao mercado e efetivamente lucrativa. Essas interven√ß√µes tamb√©m contribuem para transformar o MVP atual em um sistema base mais realista, capaz de incorporar futuras expans√µes ‚Äî como m√∫ltiplos ativos, indicadores t√©cnicos avan√ßados ou o uso de t√©cnicas Actor-Critic mais robustas.

## 8. Resultados e Discuss√£o

* 8.1 Desempenho Financeiro
* 8.2 An√°lise de Decis√µes do Agente
* 8.3 Insights Gerados

## 9. Conclus√µes e Potencial de Expans√£o

* 9.1 Resumo dos Resultados Obtidos
* 9.2 Limita√ß√µes Encontradas
* 9.3 Sugest√µes para Melhoria e Expans√£o (novos ativos, estrat√©gias avan√ßadas)
* 9.4 Considera√ß√µes Finais e Pr√≥ximos Passos

## Refer√™ncias

* Bibliografia
* Fontes de Dados
* Documenta√ß√£o Utilizada

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa322d72",
   "metadata": {},
   "source": [
    "## 8. Otimiza√ß√£o Progressiva do Agente de Reinforcement Learning\n",
    "\n",
    "Este notebook d√° in√≠cio √† **segunda fase do projeto**, na qual realizaremos ajustes espec√≠ficos com base nas an√°lises conduzidas na Se√ß√£o 7.3.  \n",
    "O objetivo desta nova etapa √© **transformar o agente atual ‚Äî que apresenta comportamento neutro ‚Äî em um agente capaz de capturar oportunidades reais de mercado**, aumentando consistentemente seu retorno ajustado ao risco.\n",
    "\n",
    "### üîç Contexto Atual\n",
    "\n",
    "At√© o momento, implementamos e treinamos um agente DQN simples, que atua sobre uma carteira composta por VALE3, PETR4 e BRFS3. Embora o agente tenha demonstrado estabilidade e preserva√ß√£o do capital, seus resultados no conjunto de valida√ß√£o indicam que:\n",
    "\n",
    "- O **lucro total m√©dio por epis√≥dio foi inferior a R$ 10,00**, o que representa rentabilidade pr√≥xima de zero;\n",
    "- O **√≠ndice de Sharpe ficou em torno de 0,06**, sinalizando retorno praticamente nulo em rela√ß√£o √† volatilidade;\n",
    "- O agente parece ter aprendido a **n√£o operar**, o que evita preju√≠zos, mas tamb√©m reduz qualquer possibilidade de ganho real.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objetivo desta Fase\n",
    "\n",
    "Esta se√ß√£o √© dedicada √† implementa√ß√£o progressiva dos seguintes ajustes, todos descritos tecnicamente na Se√ß√£o 7.4 do relat√≥rio:\n",
    "\n",
    "---\n",
    "\n",
    "### üîß **Ajustes no Ambiente de Simula√ß√£o**\n",
    "\n",
    "1. **Inserir penalidade por inatividade prolongada**\n",
    "   - Criar mecanismo que penalize epis√≥dios com longas sequ√™ncias de a√ß√µes `[0, 0, 0]`.\n",
    "   - Motivo: for√ßar o agente a avaliar se manter-se passivo √© realmente a melhor decis√£o.\n",
    "\n",
    "2. **Incluir custos de transa√ß√£o**\n",
    "   - Penaliza√ß√£o proporcional ao valor negociado em cada compra e venda.\n",
    "   - Exemplo: 0,1% do valor de cada opera√ß√£o.\n",
    "\n",
    "3. **Ampliar espa√ßo de a√ß√£o para m√∫ltiplas unidades**\n",
    "   - Permitir que o agente compre/venda at√© 3 unidades por ativo.\n",
    "   - Novas a√ß√µes: `[0, 1, 2, 3]` ‚Üí manter, comprar 1, comprar 2, comprar 3 (idem para venda).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Otimiza√ß√µes no Agente DQN**\n",
    "\n",
    "4. **Refinar fun√ß√£o de recompensa**\n",
    "   - Testar vers√µes normalizadas (porcentagem) ou baseadas em Sharpe Ratio incremental.\n",
    "\n",
    "5. **Alterar fun√ß√£o de perda**\n",
    "   - Substituir MSE por **Huber Loss**, para maior robustez a outliers.\n",
    "\n",
    "6. **Adicionar Dropout ou BatchNorm na rede**\n",
    "   - Evitar overfitting e estabilizar treinamento com camadas adicionais.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà **Estrat√©gias de Treinamento**\n",
    "\n",
    "7. **Estender o n√∫mero de epis√≥dios para 1.000+**\n",
    "   - Observar se a pol√≠tica converge com maior estabilidade.\n",
    "\n",
    "8. **Aplicar pol√≠tica Œµ-greedy com decaimento mais lento**\n",
    "   - Usar decaimento linear ou patamar fixo at√© epis√≥dio 100.\n",
    "\n",
    "9. **Inserir checkpoints de avalia√ß√£o durante o treino**\n",
    "   - Verificar a cada 100 epis√≥dios o desempenho do agente no conjunto de valida√ß√£o.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Plano de Execu√ß√£o\n",
    "\n",
    "Cada um desses ajustes ser√° implementado **de forma incremental e controlada**, com:\n",
    "- Justificativa t√©cnica;\n",
    "- Altera√ß√µes no c√≥digo;\n",
    "- Avalia√ß√£o quantitativa do impacto;\n",
    "- Registro em Markdown para documenta√ß√£o final.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Ponto de Partida\n",
    "\n",
    "Voc√™ deve:\n",
    "1. Reabrir este notebook no Google Colab com GPU (T4);\n",
    "2. Executar as c√©lulas de importa√ß√£o de dados e defini√ß√£o da vers√£o atual do ambiente;\n",
    "3. Seguir para a implementa√ß√£o do **Ajuste 1: penalidade por inatividade**.\n",
    "\n",
    "‚ö†Ô∏è **N√£o recomece do zero.** Este notebook continua o projeto j√° treinado e avaliado. Todos os resultados da fase anterior devem ser preservados para compara√ß√£o futura.\n",
    "\n",
    "---\n",
    "\n",
    "üîú **Pr√≥xima c√©lula**: redefinir a classe `PortfolioEnv` com suporte √† penalidade por inatividade.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
